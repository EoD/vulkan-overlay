From 677d6e1ec799a240e88c90f44e2ce5b988f9600d Mon Sep 17 00:00:00 2001
From: "Luke A. Guest" <laguest@archeia.com>
Date: Fri, 3 Feb 2017 16:17:24 +0000
Subject: [PATCH 14/14] Change fence types to dma_fence, for kernel v4.10.

---
 amd/amdgpu/amdgpu.h                |  60 ++++++++++++++++++
 amd/amdgpu/amdgpu_benchmark.c      |  16 +++++
 amd/amdgpu/amdgpu_cs.c             |  57 +++++++++++++++++
 amd/amdgpu/amdgpu_ctx.c            |  39 +++++++++++-
 amd/amdgpu/amdgpu_device.c         |  24 +++++++
 amd/amdgpu/amdgpu_display.c        |  28 ++++++++
 amd/amdgpu/amdgpu_fence.c          |  87 +++++++++++++++++++++++++
 amd/amdgpu/amdgpu_ib.c             |  10 +++
 amd/amdgpu/amdgpu_job.c            |  39 ++++++++++++
 amd/amdgpu/amdgpu_object.c         |  30 +++++++++
 amd/amdgpu/amdgpu_object.h         |  20 +++++-
 amd/amdgpu/amdgpu_ring.h           |   8 +++
 amd/amdgpu/amdgpu_sa.c             |  45 +++++++++++++
 amd/amdgpu/amdgpu_sem.c            |  33 ++++++++++
 amd/amdgpu/amdgpu_sem.h            |   8 +++
 amd/amdgpu/amdgpu_sync.c           |  93 +++++++++++++++++++++++++++
 amd/amdgpu/amdgpu_sync.h           |  18 ++++++
 amd/amdgpu/amdgpu_test.c           |  24 +++++++
 amd/amdgpu/amdgpu_trace.h          |   8 +++
 amd/amdgpu/amdgpu_ttm.c            |  37 ++++++++++-
 amd/amdgpu/amdgpu_ttm.h            |  11 +++-
 amd/amdgpu/amdgpu_uvd.c            |  51 +++++++++++++++
 amd/amdgpu/amdgpu_uvd.h            |  11 +++-
 amd/amdgpu/amdgpu_vce.c            |  50 +++++++++++++++
 amd/amdgpu/amdgpu_vce.h            |  11 +++-
 amd/amdgpu/amdgpu_vm.c             | 127 ++++++++++++++++++++++++++++++++++++-
 amd/amdgpu/amdgpu_vm.h             |  23 ++++++-
 amd/amdgpu/cik_sdma.c              |  12 ++++
 amd/amdgpu/gfx_v6_0.c              |  12 ++++
 amd/amdgpu/gfx_v7_0.c              |  12 ++++
 amd/amdgpu/gfx_v8_0.c              |  24 +++++++
 amd/amdgpu/sdma_v2_4.c             |  12 ++++
 amd/amdgpu/sdma_v3_0.c             |  12 ++++
 amd/amdgpu/si_dma.c                |  12 ++++
 amd/amdkcl/kcl_fence.c             |  93 +++++++++++++++++++++++++++
 amd/amdkcl/kcl_reservation.c       |  40 ++++++++++++
 amd/scheduler/gpu_sched_trace.h    |   8 +++
 amd/scheduler/gpu_scheduler.c      | 102 +++++++++++++++++++++++++++++
 amd/scheduler/gpu_scheduler.h      |  32 ++++++++++
 amd/scheduler/sched_fence.c        |  58 +++++++++++++++++
 include/drm/ttm/ttm_bo_api.h       |   4 ++
 include/drm/ttm/ttm_bo_driver.h    |  18 +++++-
 include/drm/ttm/ttm_execbuf_util.h |   4 ++
 include/kcl/kcl_fence.h            |  39 +++++++++++-
 include/kcl/kcl_fence_array.h      |  52 +++++++++++++++
 ttm/ttm_bo.c                       |  45 +++++++++++++
 ttm/ttm_bo_util.c                  |  33 +++++++++-
 ttm/ttm_bo_vm.c                    |  16 +++++
 ttm/ttm_execbuf_util.c             |   7 +-
 49 files changed, 1602 insertions(+), 13 deletions(-)

diff --git a/amd/amdgpu/amdgpu.h b/amd/amdgpu/amdgpu.h
index bffe16e..83a51a3 100644
--- a/amd/amdgpu/amdgpu.h
+++ b/amd/amdgpu/amdgpu.h
@@ -34,7 +34,11 @@
 #include <linux/kref.h>
 #include <linux/interval_tree.h>
 #include <linux/hashtable.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+#include <linux/dma-fence.h>
+#else
 #include <linux/fence.h>
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 #include <ttm/ttm_bo_api.h>
 #include <ttm/ttm_bo_driver.h>
@@ -362,7 +366,11 @@ struct amdgpu_bo_va_mapping {
 struct amdgpu_bo_va {
 	/* protected by bo being reserved */
 	struct list_head		bo_list;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	        *last_pt_update;
+#else
 	struct fence		        *last_pt_update;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned			ref_count;
 
 	/* protected by vm mutex and spinlock */
@@ -478,7 +486,11 @@ struct amdgpu_sa_bo {
 	struct amdgpu_sa_manager	*manager;
 	unsigned			soffset;
 	unsigned			eoffset;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	        *fence;
+#else
 	struct fence		        *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 };
 
 /*
@@ -616,10 +628,19 @@ struct amdgpu_flip_work {
 	uint64_t			base;
 	struct drm_pending_vblank_event *event;
 	struct amdgpu_bo		*old_abo;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence		*excl;
+#else
 	struct fence			*excl;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned			shared_count;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence		**shared;
+	struct dma_fence_cb		cb;
+#else
 	struct fence			**shared;
 	struct fence_cb			cb;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	bool				async;
 };
 
@@ -647,7 +668,11 @@ void amdgpu_job_free_resources(struct amdgpu_job *job);
 void amdgpu_job_free(struct amdgpu_job *job);
 int amdgpu_job_submit(struct amdgpu_job *job, struct amdgpu_ring *ring,
 		      struct amd_sched_entity *entity, void *owner,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		      struct dma_fence **f);
+#else
 		      struct fence **f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 /*
  * context related structures
@@ -655,7 +680,11 @@ int amdgpu_job_submit(struct amdgpu_job *job, struct amdgpu_ring *ring,
 
 struct amdgpu_ctx_ring {
 	uint64_t		sequence;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	**fences;
+#else
 	struct fence		**fences;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct amd_sched_entity	entity;
 	struct list_head	sem_list;
 	struct mutex            sem_lock;
@@ -666,7 +695,11 @@ struct amdgpu_ctx {
 	struct amdgpu_device    *adev;
 	unsigned		reset_counter;
 	spinlock_t		ring_lock;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence        **fences;
+#else
 	struct fence            **fences;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct amdgpu_ctx_ring	rings[AMDGPU_MAX_RINGS];
 	bool preamble_presented;
 };
@@ -681,10 +714,17 @@ struct amdgpu_ctx_mgr {
 struct amdgpu_ctx *amdgpu_ctx_get(struct amdgpu_fpriv *fpriv, uint32_t id);
 int amdgpu_ctx_put(struct amdgpu_ctx *ctx);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+uint64_t amdgpu_ctx_add_fence(struct amdgpu_ctx *ctx, struct amdgpu_ring *ring,
+			      struct dma_fence *fence);
+struct dma_fence *amdgpu_ctx_get_fence(struct amdgpu_ctx *ctx,
+				       struct amdgpu_ring *ring, uint64_t seq);
+#else
 uint64_t amdgpu_ctx_add_fence(struct amdgpu_ctx *ctx, struct amdgpu_ring *ring,
 			      struct fence *fence);
 struct fence *amdgpu_ctx_get_fence(struct amdgpu_ctx *ctx,
 				   struct amdgpu_ring *ring, uint64_t seq);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 int amdgpu_ctx_ioctl(struct drm_device *dev, void *data,
 		     struct drm_file *filp);
@@ -895,11 +935,23 @@ struct amdgpu_gfx {
 
 int amdgpu_ib_get(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 		  unsigned size, struct amdgpu_ib *ib);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+struct dma_fence *amdgpu_ctx_get_fence(struct amdgpu_ctx *ctx,
+				   struct amdgpu_ring *ring, uint64_t seq);
+void amdgpu_ib_free(struct amdgpu_device *adev, struct amdgpu_ib *ib,
+		    struct dma_fence *f);
+int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
+		       struct amdgpu_ib *ib, struct dma_fence *last_vm_update,
+		       struct amdgpu_job *job, struct dma_fence **f);
+#else
+struct fence *amdgpu_ctx_get_fence(struct amdgpu_ctx *ctx,
+				   struct amdgpu_ring *ring, uint64_t seq);
 void amdgpu_ib_free(struct amdgpu_device *adev, struct amdgpu_ib *ib,
 		    struct fence *f);
 int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
 		       struct amdgpu_ib *ib, struct fence *last_vm_update,
 		       struct amdgpu_job *job, struct fence **f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 int amdgpu_ib_pool_init(struct amdgpu_device *adev);
 void amdgpu_ib_pool_fini(struct amdgpu_device *adev);
 int amdgpu_ib_ring_tests(struct amdgpu_device *adev);
@@ -930,7 +982,11 @@ struct amdgpu_cs_parser {
 	struct amdgpu_bo_list		*bo_list;
 	struct amdgpu_bo_list_entry	vm_pd;
 	struct list_head		validated;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence		*fence;
+#else
 	struct fence			*fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	uint64_t			bytes_moved_threshold;
 	uint64_t			bytes_moved;
 	struct amdgpu_bo_list_entry	*evictable;
@@ -950,7 +1006,11 @@ struct amdgpu_job {
 	struct amdgpu_ring	*ring;
 	struct amdgpu_sync	sync;
 	struct amdgpu_ib	*ibs;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	*fence; /* the hw fence */
+#else
 	struct fence		*fence; /* the hw fence */
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	uint32_t		preamble_status;
 	uint32_t		num_ibs;
 	void			*owner;
diff --git a/amd/amdgpu/amdgpu_benchmark.c b/amd/amdgpu/amdgpu_benchmark.c
index 3453052..b627b6d 100644
--- a/amd/amdgpu/amdgpu_benchmark.c
+++ b/amd/amdgpu/amdgpu_benchmark.c
@@ -33,7 +33,11 @@ static int amdgpu_benchmark_do_move(struct amdgpu_device *adev, unsigned size,
 {
 	unsigned long start_jiffies;
 	unsigned long end_jiffies;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence = NULL;
+#else
 	struct fence *fence = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int i, r;
 
 	start_jiffies = jiffies;
@@ -43,17 +47,29 @@ static int amdgpu_benchmark_do_move(struct amdgpu_device *adev, unsigned size,
 				       false);
 		if (r)
 			goto exit_do_move;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		r = dma_fence_wait(fence, false);
+#else
 		r = fence_wait(fence, false);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (r)
 			goto exit_do_move;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	}
 	end_jiffies = jiffies;
 	r = jiffies_to_msecs(end_jiffies - start_jiffies);
 
 exit_do_move:
 	if (fence)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return r;
 }
 
diff --git a/amd/amdgpu/amdgpu_cs.c b/amd/amdgpu/amdgpu_cs.c
index 45e04df..5f4f298 100644
--- a/amd/amdgpu/amdgpu_cs.c
+++ b/amd/amdgpu/amdgpu_cs.c
@@ -736,7 +736,11 @@ static void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser, int error, bo
 		ttm_eu_backoff_reservation(&parser->ticket,
 					   &parser->validated);
 	}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(parser->fence);
+#else
 	fence_put(parser->fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	if (parser->ctx)
 		amdgpu_ctx_put(parser->ctx);
@@ -773,7 +777,11 @@ static int amdgpu_bo_vm_update_pte(struct amdgpu_cs_parser *p,
 
 	if (p->bo_list) {
 		for (i = 0; i < p->bo_list->num_entries; i++) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			struct dma_fence *f;
+#else
 			struct fence *f;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 			/* ignore duplicates */
 			bo = p->bo_list->array[i].robj;
@@ -964,7 +972,11 @@ static int amdgpu_cs_dependencies(struct amdgpu_device *adev,
 		for (j = 0; j < num_deps; ++j) {
 			struct amdgpu_ring *ring;
 			struct amdgpu_ctx *ctx;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			struct dma_fence *fence;
+#else
 			struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 			r = amdgpu_cs_get_ring(adev, deps[j].ip_type,
 					       deps[j].ip_instance,
@@ -986,7 +998,11 @@ static int amdgpu_cs_dependencies(struct amdgpu_device *adev,
 			} else if (fence) {
 				r = amdgpu_sync_fence(adev, &p->job->sync,
 						      fence);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				dma_fence_put(fence);
+#else
 				fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 				amdgpu_ctx_put(ctx);
 				if (r)
 					return r;
@@ -1016,7 +1032,11 @@ static int amdgpu_cs_submit(struct amdgpu_cs_parser *p,
 
 	job->owner = p->filp;
 	job->fence_ctx = entity->fence_context;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	p->fence = dma_fence_get(&job->base.s_fence->finished);
+#else
 	p->fence = fence_get(&job->base.s_fence->finished);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	cs->out.handle = amdgpu_ctx_add_fence(p->ctx, ring, p->fence);
 	job->uf_sequence = cs->out.handle;
 	amdgpu_job_free_resources(job);
@@ -1098,7 +1118,11 @@ int amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,
 	unsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);
 	struct amdgpu_ring *ring = NULL;
 	struct amdgpu_ctx *ctx;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence;
+#else
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	long r;
 
 	r = amdgpu_cs_get_ring(adev, wait->in.ip_type, wait->in.ip_instance,
@@ -1115,7 +1139,11 @@ int amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,
 		r = PTR_ERR(fence);
 	else if (fence) {
 		r = kcl_fence_wait_timeout(fence, true, timeout);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	} else
 		r = 1;
 
@@ -1136,13 +1164,21 @@ int amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,
  * @filp: file private
  * @user: drm_amdgpu_fence copied from user space
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,
+#else
 static struct fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 					 struct drm_file *filp,
 					 struct drm_amdgpu_fence *user)
 {
 	struct amdgpu_ring *ring;
 	struct amdgpu_ctx *ctx;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence;
+#else
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int r;
 
 	r = amdgpu_cs_get_ring(adev, user->ip_type, user->ip_instance,
@@ -1178,7 +1214,11 @@ static int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,
 	long r = 1;
 
 	for (i = 0; i < fence_count; i++) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence;
+#else
 		struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		unsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);
 
 		fence = amdgpu_cs_get_fence(adev, filp, &fences[i]);
@@ -1217,18 +1257,31 @@ static int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,
 	unsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);
 	uint32_t fence_count = wait->in.fence_count;
 	uint32_t first = ~0;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence **array;
+#else
 	struct fence **array;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned i;
 	long r;
 
 	/* Prepare the fence array */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	array = (struct dma_fence **)kcalloc(fence_count, sizeof(struct dma_fence *),
+			GFP_KERNEL);
+#else
 	array = (struct fence **)kcalloc(fence_count, sizeof(struct fence *),
 			GFP_KERNEL);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (array == NULL)
 		return -ENOMEM;
 
 	for (i = 0; i < fence_count; i++) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence;
+#else
 		struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		fence = amdgpu_cs_get_fence(adev, filp, &fences[i]);
 		if (IS_ERR(fence)) {
@@ -1255,7 +1308,11 @@ out:
 
 err_free_fence_array:
 	for (i = 0; i < fence_count; i++)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(array[i]);
+#else
 		fence_put(array[i]);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	kfree(array);
 
 	return r;
diff --git a/amd/amdgpu/amdgpu_ctx.c b/amd/amdgpu/amdgpu_ctx.c
index d7df6e0..e2f0428 100644
--- a/amd/amdgpu/amdgpu_ctx.c
+++ b/amd/amdgpu/amdgpu_ctx.c
@@ -34,8 +34,13 @@ static int amdgpu_ctx_init(struct amdgpu_device *adev, struct amdgpu_ctx *ctx)
 	ctx->adev = adev;
 	kref_init(&ctx->refcount);
 	spin_lock_init(&ctx->ring_lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	ctx->fences = kcalloc(amdgpu_sched_jobs * AMDGPU_MAX_RINGS,
+			      sizeof(struct dma_fence*), GFP_KERNEL);
+#else
 	ctx->fences = kcalloc(amdgpu_sched_jobs * AMDGPU_MAX_RINGS,
 			      sizeof(struct fence*), GFP_KERNEL);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (!ctx->fences)
 		return -ENOMEM;
 
@@ -80,7 +85,11 @@ static void amdgpu_ctx_fini(struct amdgpu_ctx *ctx)
 
 	for (i = 0; i < AMDGPU_MAX_RINGS; ++i)
 		for (j = 0; j < amdgpu_sched_jobs; ++j) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_put(ctx->rings[i].fences[j]);
+#else
 			fence_put(ctx->rings[i].fences[j]);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			mutex_destroy(&ctx->rings[i].sem_lock);
 		}
 	kfree(ctx->fences);
@@ -243,13 +252,22 @@ int amdgpu_ctx_put(struct amdgpu_ctx *ctx)
 	return 0;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+uint64_t amdgpu_ctx_add_fence(struct amdgpu_ctx *ctx, struct amdgpu_ring *ring,
+			      struct dma_fence *fence)
+#else
 uint64_t amdgpu_ctx_add_fence(struct amdgpu_ctx *ctx, struct amdgpu_ring *ring,
 			      struct fence *fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_ctx_ring *cring = & ctx->rings[ring->idx];
 	uint64_t seq = cring->sequence;
 	unsigned idx = 0;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *other = NULL;
+#else
 	struct fence *other = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	idx = seq & (amdgpu_sched_jobs - 1);
 	other = cring->fences[idx];
@@ -260,23 +278,38 @@ uint64_t amdgpu_ctx_add_fence(struct amdgpu_ctx *ctx, struct amdgpu_ring *ring,
 			DRM_ERROR("Error (%ld) waiting for fence!\n", r);
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_get(fence);
+#else
 	fence_get(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	spin_lock(&ctx->ring_lock);
 	cring->fences[idx] = fence;
 	cring->sequence++;
 	spin_unlock(&ctx->ring_lock);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(other);
+#else
 	fence_put(other);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	return seq;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+struct dma_fence *amdgpu_ctx_get_fence(struct amdgpu_ctx *ctx,
+				   struct amdgpu_ring *ring, uint64_t seq)
+{
+	struct dma_fence *fence;
+#else
 struct fence *amdgpu_ctx_get_fence(struct amdgpu_ctx *ctx,
 				   struct amdgpu_ring *ring, uint64_t seq)
 {
-	struct amdgpu_ctx_ring *cring = & ctx->rings[ring->idx];
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
+	struct amdgpu_ctx_ring *cring = & ctx->rings[ring->idx];
 
 	spin_lock(&ctx->ring_lock);
 
@@ -291,7 +324,11 @@ struct fence *amdgpu_ctx_get_fence(struct amdgpu_ctx *ctx,
 		return NULL;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	fence = dma_fence_get(cring->fences[seq & (amdgpu_sched_jobs - 1)]);
+#else
 	fence = fence_get(cring->fences[seq & (amdgpu_sched_jobs - 1)]);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	spin_unlock(&ctx->ring_lock);
 
 	return fence;
diff --git a/amd/amdgpu/amdgpu_device.c b/amd/amdgpu/amdgpu_device.c
index b7f17c2..a064ef6 100644
--- a/amd/amdgpu/amdgpu_device.c
+++ b/amd/amdgpu/amdgpu_device.c
@@ -2272,7 +2272,11 @@ bool amdgpu_need_backup(struct amdgpu_device *adev)
 static int amdgpu_recover_vram_from_shadow(struct amdgpu_device *adev,
 					   struct amdgpu_ring *ring,
 					   struct amdgpu_bo *bo,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+					   struct dma_fence **fence)
+#else
 					   struct fence **fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	uint32_t domain;
 	int r;
@@ -2392,30 +2396,50 @@ retry:
 		if (need_full_reset && amdgpu_need_backup(adev)) {
 			struct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;
 			struct amdgpu_bo *bo, *tmp;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			struct dma_fence *fence = NULL, *next = NULL;
+#else
 			struct fence *fence = NULL, *next = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 			DRM_INFO("recover vram bo from shadow\n");
 			mutex_lock(&adev->shadow_list_lock);
 			list_for_each_entry_safe(bo, tmp, &adev->shadow_list, shadow_list) {
 				amdgpu_recover_vram_from_shadow(adev, ring, bo, &next);
 				if (fence) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+					r = dma_fence_wait(fence, false);
+#else
 					r = fence_wait(fence, false);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 					if (r) {
 						WARN(r, "recovery from shadow isn't comleted\n");
 						break;
 					}
 				}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				dma_fence_put(fence);
+#else
 				fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 				fence = next;
 			}
 			mutex_unlock(&adev->shadow_list_lock);
 			if (fence) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				r = dma_fence_wait(fence, false);
+#else
 				r = fence_wait(fence, false);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 				if (r)
 					WARN(r, "recovery from shadow isn't comleted\n");
 			}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_put(fence);
+#else
 			fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		}
 		for (i = 0; i < AMDGPU_MAX_RINGS; ++i) {
 			struct amdgpu_ring *ring = adev->rings[i];
diff --git a/amd/amdgpu/amdgpu_display.c b/amd/amdgpu/amdgpu_display.c
index 4e39853..b20cc11 100644
--- a/amd/amdgpu/amdgpu_display.c
+++ b/amd/amdgpu/amdgpu_display.c
@@ -35,29 +35,51 @@
 #include <drm/drm_crtc_helper.h>
 #include <drm/drm_edid.h>
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amdgpu_flip_callback(struct dma_fence *f, struct dma_fence_cb *cb)
+#else
 static void amdgpu_flip_callback(struct fence *f, struct fence_cb *cb)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_flip_work *work =
 		container_of(cb, struct amdgpu_flip_work, cb);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	schedule_work(&work->flip_work);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static bool amdgpu_flip_handle_fence(struct amdgpu_flip_work *work,
+				     struct dma_fence **f)
+{
+	struct dma_fence *fence= *f;
+#else
 static bool amdgpu_flip_handle_fence(struct amdgpu_flip_work *work,
 				     struct fence **f)
 {
 	struct fence *fence= *f;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	if (fence == NULL)
 		return false;
 
 	*f = NULL;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (!dma_fence_add_callback(fence, &work->cb, amdgpu_flip_callback))
+		return true;
+
+	dma_fence_put(fence);
+#else
 	if (!fence_add_callback(fence, &work->cb, amdgpu_flip_callback))
 		return true;
 
 	fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return false;
 }
 
@@ -312,9 +334,15 @@ unreserve:
 
 cleanup:
 	amdgpu_bo_unref(&work->old_abo);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(work->excl);
+	for (i = 0; i < work->shared_count; ++i)
+		dma_fence_put(work->shared[i]);
+#else
 	fence_put(work->excl);
 	for (i = 0; i < work->shared_count; ++i)
 		fence_put(work->shared[i]);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	kfree(work->shared);
 	kfree(work);
 
diff --git a/amd/amdgpu/amdgpu_fence.c b/amd/amdgpu/amdgpu_fence.c
index 2d23200..541728f 100644
--- a/amd/amdgpu/amdgpu_fence.c
+++ b/amd/amdgpu/amdgpu_fence.c
@@ -48,7 +48,11 @@
  */
 
 struct amdgpu_fence {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence base;
+#else
 	struct fence base;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	/* RB, DMA, etc. */
 	struct amdgpu_ring		*ring;
@@ -74,8 +78,13 @@ void amdgpu_fence_slab_fini(void)
 /*
  * Cast helper
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static const struct dma_fence_ops amdgpu_fence_ops;
+static inline struct amdgpu_fence *to_amdgpu_fence(struct dma_fence *f)
+#else
 static const struct fence_ops amdgpu_fence_ops;
 static inline struct amdgpu_fence *to_amdgpu_fence(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_fence *__f = container_of(f, struct amdgpu_fence, base);
 
@@ -131,11 +140,19 @@ static u32 amdgpu_fence_read(struct amdgpu_ring *ring)
  * Emits a fence command on the requested ring (all asics).
  * Returns 0 on success, -ENOMEM on failure.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+int amdgpu_fence_emit(struct amdgpu_ring *ring, struct dma_fence **f)
+#else
 int amdgpu_fence_emit(struct amdgpu_ring *ring, struct fence **f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_fence *fence;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *old, **ptr;
+#else
 	struct fence *old, **ptr;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	uint32_t seq;
 
 	fence = kmem_cache_alloc(amdgpu_fence_slab, GFP_KERNEL);
@@ -156,12 +173,21 @@ int amdgpu_fence_emit(struct amdgpu_ring *ring, struct fence **f)
 	 * emitting the fence would mess up the hardware ring buffer.
 	 */
 	old = rcu_dereference_protected(*ptr, 1);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (old && !dma_fence_is_signaled(old)) {
+		DRM_INFO("rcu slot is busy\n");
+		dma_fence_wait(old, false);
+	}
+
+	rcu_assign_pointer(*ptr, dma_fence_get(&fence->base));
+#else
 	if (old && !fence_is_signaled(old)) {
 		DRM_INFO("rcu slot is busy\n");
 		fence_wait(old, false);
 	}
 
 	rcu_assign_pointer(*ptr, fence_get(&fence->base));
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	*f = &fence->base;
 
@@ -212,7 +238,11 @@ void amdgpu_fence_process(struct amdgpu_ring *ring)
 	seq &= drv->num_fences_mask;
 
 	do {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence, **ptr;
+#else
 		struct fence *fence, **ptr;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		++last_seq;
 		last_seq &= drv->num_fences_mask;
@@ -225,13 +255,25 @@ void amdgpu_fence_process(struct amdgpu_ring *ring)
 		if (!fence)
 			continue;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		r = dma_fence_signal(fence);
+#else
 		r = fence_signal(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (!r)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			DMA_FENCE_TRACE(fence, "signaled from irq context\n");
+#else
 			FENCE_TRACE(fence, "signaled from irq context\n");
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		else
 			BUG();
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	} while (last_seq != seq);
 }
 
@@ -261,7 +303,11 @@ static void amdgpu_fence_fallback(unsigned long arg)
 int amdgpu_fence_wait_empty(struct amdgpu_ring *ring)
 {
 	uint64_t seq = ACCESS_ONCE(ring->fence_drv.sync_seq);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence, **ptr;
+#else
 	struct fence *fence, **ptr;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int r;
 
 	if (!seq)
@@ -270,14 +316,23 @@ int amdgpu_fence_wait_empty(struct amdgpu_ring *ring)
 	ptr = &ring->fence_drv.fences[seq & ring->fence_drv.num_fences_mask];
 	rcu_read_lock();
 	fence = rcu_dereference(*ptr);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (!fence || !dma_fence_get_rcu(fence)) {
+#else
 	if (!fence || !fence_get_rcu(fence)) {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		rcu_read_unlock();
 		return 0;
 	}
 	rcu_read_unlock();
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait(fence, false);
+	dma_fence_put(fence);
+#else
 	r = fence_wait(fence, false);
 	fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return r;
 }
 
@@ -456,7 +511,11 @@ void amdgpu_fence_driver_fini(struct amdgpu_device *adev)
 		amd_sched_fini(&ring->sched);
 		del_timer_sync(&ring->fence_drv.fallback_timer);
 		for (j = 0; j <= ring->fence_drv.num_fences_mask; ++j)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_put(ring->fence_drv.fences[j]);
+#else
 			fence_put(ring->fence_drv.fences[j]);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		kfree(ring->fence_drv.fences);
 		ring->fence_drv.fences = NULL;
 		ring->fence_drv.initialized = false;
@@ -545,12 +604,20 @@ void amdgpu_fence_driver_force_completion(struct amdgpu_device *adev)
  * Common fence implementation
  */
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static const char *amdgpu_fence_get_driver_name(struct dma_fence *fence)
+#else
 static const char *amdgpu_fence_get_driver_name(struct fence *fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	return "amdgpu";
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static const char *amdgpu_fence_get_timeline_name(struct dma_fence *f)
+#else
 static const char *amdgpu_fence_get_timeline_name(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_fence *fence = to_amdgpu_fence(f);
 	return (const char *)fence->ring->name;
@@ -564,7 +631,11 @@ static const char *amdgpu_fence_get_timeline_name(struct fence *f)
  * to fence_queue that checks if this fence is signaled, and if so it
  * signals the fence and removes itself.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static bool amdgpu_fence_enable_signaling(struct dma_fence *f)
+#else
 static bool amdgpu_fence_enable_signaling(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_fence *fence = to_amdgpu_fence(f);
 	struct amdgpu_ring *ring = fence->ring;
@@ -572,7 +643,11 @@ static bool amdgpu_fence_enable_signaling(struct fence *f)
 	if (!timer_pending(&ring->fence_drv.fallback_timer))
 		amdgpu_fence_schedule_fallback(ring);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	DMA_FENCE_TRACE(&fence->base, "armed on ring %i!\n", ring->idx);
+#else
 	FENCE_TRACE(&fence->base, "armed on ring %i!\n", ring->idx);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	return true;
 }
@@ -586,7 +661,11 @@ static bool amdgpu_fence_enable_signaling(struct fence *f)
  */
 static void amdgpu_fence_free(struct rcu_head *rcu)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = container_of(rcu, struct dma_fence, rcu);
+#else
 	struct fence *f = container_of(rcu, struct fence, rcu);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct amdgpu_fence *fence = to_amdgpu_fence(f);
 	kmem_cache_free(amdgpu_fence_slab, fence);
 }
@@ -599,12 +678,20 @@ static void amdgpu_fence_free(struct rcu_head *rcu)
  * This function is called when the reference count becomes zero.
  * It just RCU schedules freeing up the fence.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amdgpu_fence_release(struct dma_fence *f)
+#else
 static void amdgpu_fence_release(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	call_rcu(&f->rcu, amdgpu_fence_free);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static const struct dma_fence_ops amdgpu_fence_ops = {
+#else
 static const struct fence_ops amdgpu_fence_ops = {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	.get_driver_name = amdgpu_fence_get_driver_name,
 	.get_timeline_name = amdgpu_fence_get_timeline_name,
 	.enable_signaling = amdgpu_fence_enable_signaling,
diff --git a/amd/amdgpu/amdgpu_ib.c b/amd/amdgpu/amdgpu_ib.c
index 0bbf31e..588cf92 100644
--- a/amd/amdgpu/amdgpu_ib.c
+++ b/amd/amdgpu/amdgpu_ib.c
@@ -89,7 +89,11 @@ int amdgpu_ib_get(struct amdgpu_device *adev, struct amdgpu_vm *vm,
  * Free an IB (all asics).
  */
 void amdgpu_ib_free(struct amdgpu_device *adev, struct amdgpu_ib *ib,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		    struct dma_fence *f)
+#else
 		    struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	amdgpu_sa_bo_free(adev, &ib->sa_bo, f);
 }
@@ -115,9 +119,15 @@ void amdgpu_ib_free(struct amdgpu_device *adev, struct amdgpu_ib *ib,
  * a CONST_IB), it will be put on the ring prior to the DE IB.  Prior
  * to SI there was just a DE IB.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
+		       struct amdgpu_ib *ibs, struct dma_fence *last_vm_update,
+		       struct amdgpu_job *job, struct dma_fence **f)
+#else
 int amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned num_ibs,
 		       struct amdgpu_ib *ibs, struct fence *last_vm_update,
 		       struct amdgpu_job *job, struct fence **f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib *ib = &ibs[0];
diff --git a/amd/amdgpu/amdgpu_job.c b/amd/amdgpu/amdgpu_job.c
index 8c58079..b652b56 100644
--- a/amd/amdgpu/amdgpu_job.c
+++ b/amd/amdgpu/amdgpu_job.c
@@ -81,7 +81,11 @@ int amdgpu_job_alloc_with_ib(struct amdgpu_device *adev, unsigned size,
 
 void amdgpu_job_free_resources(struct amdgpu_job *job)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f;
+#else
 	struct fence *f;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned i;
 
 	/* use sched fence if available */
@@ -95,7 +99,11 @@ static void amdgpu_job_free_cb(struct amd_sched_job *s_job)
 {
 	struct amdgpu_job *job = container_of(s_job, struct amdgpu_job, base);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(job->fence);
+#else
 	fence_put(job->fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	amdgpu_sync_free(&job->sync);
 	kfree(job);
 }
@@ -104,14 +112,22 @@ void amdgpu_job_free(struct amdgpu_job *job)
 {
 	amdgpu_job_free_resources(job);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(job->fence);
+#else
 	fence_put(job->fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	amdgpu_sync_free(&job->sync);
 	kfree(job);
 }
 
 int amdgpu_job_submit(struct amdgpu_job *job, struct amdgpu_ring *ring,
 		      struct amd_sched_entity *entity, void *owner,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		      struct dma_fence **f)
+#else
 		      struct fence **f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	int r;
 	job->ring = ring;
@@ -125,19 +141,31 @@ int amdgpu_job_submit(struct amdgpu_job *job, struct amdgpu_ring *ring,
 
 	job->owner = owner;
 	job->fence_ctx = entity->fence_context;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	*f = dma_fence_get(&job->base.s_fence->finished);
+#else
 	*f = fence_get(&job->base.s_fence->finished);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	amdgpu_job_free_resources(job);
 	amd_sched_entity_push_job(&job->base);
 
 	return 0;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static struct dma_fence *amdgpu_job_dependency(struct amd_sched_job *sched_job)
+#else
 static struct fence *amdgpu_job_dependency(struct amd_sched_job *sched_job)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_job *job = to_amdgpu_job(sched_job);
 	struct amdgpu_vm *vm = job->vm;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence = amdgpu_sync_get_fence(&job->sync);
+#else
 	struct fence *fence = amdgpu_sync_get_fence(&job->sync);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	if (fence == NULL && vm && !job->vm_id) {
 		struct amdgpu_ring *ring = job->ring;
@@ -155,9 +183,15 @@ static struct fence *amdgpu_job_dependency(struct amd_sched_job *sched_job)
 	return fence;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static struct dma_fence *amdgpu_job_run(struct amd_sched_job *sched_job)
+{
+	struct dma_fence *fence = NULL;
+#else
 static struct fence *amdgpu_job_run(struct amd_sched_job *sched_job)
 {
 	struct fence *fence = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct amdgpu_job *job;
 	int r;
 
@@ -176,8 +210,13 @@ static struct fence *amdgpu_job_run(struct amd_sched_job *sched_job)
 		DRM_ERROR("Error scheduling IBs (%d)\n", r);
 
 	/* if gpu reset, hw fence will be replaced here */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(job->fence);
+	job->fence = dma_fence_get(fence);
+#else
 	fence_put(job->fence);
 	job->fence = fence_get(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	amdgpu_job_free_resources(job);
 	return fence;
 }
diff --git a/amd/amdgpu/amdgpu_object.c b/amd/amdgpu/amdgpu_object.c
index ed3ce7a..ea06d85 100644
--- a/amd/amdgpu/amdgpu_object.c
+++ b/amd/amdgpu/amdgpu_object.c
@@ -428,20 +428,37 @@ int amdgpu_bo_create_restricted(struct amdgpu_device *adev,
 
 	if (flags & AMDGPU_GEM_CREATE_VRAM_CLEARED &&
 	    bo->tbo.mem.placement & TTM_PL_FLAG_VRAM) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence;
+#else
 		struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		r = amdgpu_fill_buffer(bo, 0, bo->tbo.resv, &fence);
 		if (unlikely(r))
 			goto fail_unreserve;
 
 #if defined(BUILD_AS_DKMS)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_wait(fence, false);
+#else
 		fence_wait(fence, false);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 #else
 		amdgpu_bo_fence(bo, fence, false);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(bo->tbo.moving);
+		bo->tbo.moving = dma_fence_get(fence);
+#else
 		fence_put(bo->tbo.moving);
 		bo->tbo.moving = fence_get(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 #endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	}
 	if (!resv)
 		ww_mutex_unlock(&bo->tbo.resv->lock);
@@ -537,7 +554,11 @@ int amdgpu_bo_backup_to_shadow(struct amdgpu_device *adev,
 			       struct amdgpu_ring *ring,
 			       struct amdgpu_bo *bo,
 			       struct reservation_object *resv,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			       struct dma_fence **fence,
+#else
 			       struct fence **fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			       bool direct)
 
 {
@@ -569,7 +590,11 @@ int amdgpu_bo_restore_from_shadow(struct amdgpu_device *adev,
 				  struct amdgpu_ring *ring,
 				  struct amdgpu_bo *bo,
 				  struct reservation_object *resv,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				  struct dma_fence **fence,
+#else
 				  struct fence **fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 				  bool direct)
 
 {
@@ -979,8 +1004,13 @@ int amdgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo)
  * @shared: true if fence should be added shared
  *
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+void amdgpu_bo_fence(struct amdgpu_bo *bo, struct dma_fence *fence,
+		     bool shared)
+#else
 void amdgpu_bo_fence(struct amdgpu_bo *bo, struct fence *fence,
 		     bool shared)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct reservation_object *resv = bo->tbo.resv;
 
diff --git a/amd/amdgpu/amdgpu_object.h b/amd/amdgpu/amdgpu_object.h
index f365fb7..d79cc28 100644
--- a/amd/amdgpu/amdgpu_object.h
+++ b/amd/amdgpu/amdgpu_object.h
@@ -161,19 +161,33 @@ int amdgpu_bo_get_metadata(struct amdgpu_bo *bo, void *buffer,
 void amdgpu_bo_move_notify(struct ttm_buffer_object *bo,
 				  struct ttm_mem_reg *new_mem);
 int amdgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+void amdgpu_bo_fence(struct amdgpu_bo *bo, struct dma_fence *fence,
+		     bool shared);
+#else
 void amdgpu_bo_fence(struct amdgpu_bo *bo, struct fence *fence,
 		     bool shared);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 u64 amdgpu_bo_gpu_offset(struct amdgpu_bo *bo);
 int amdgpu_bo_backup_to_shadow(struct amdgpu_device *adev,
 			       struct amdgpu_ring *ring,
 			       struct amdgpu_bo *bo,
 			       struct reservation_object *resv,
-			       struct fence **fence, bool direct);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			       struct dma_fence **fence,
+#else
+			       struct fence **fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
+			       bool direct);
 int amdgpu_bo_restore_from_shadow(struct amdgpu_device *adev,
 				  struct amdgpu_ring *ring,
 				  struct amdgpu_bo *bo,
 				  struct reservation_object *resv,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				  struct dma_fence **fence,
+#else
 				  struct fence **fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 				  bool direct);
 
 
@@ -205,7 +219,11 @@ int amdgpu_sa_bo_new(struct amdgpu_sa_manager *sa_manager,
 		     unsigned size, unsigned align);
 void amdgpu_sa_bo_free(struct amdgpu_device *adev,
 			      struct amdgpu_sa_bo **sa_bo,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			      struct dma_fence *fence);
+#else
 			      struct fence *fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 #if defined(CONFIG_DEBUG_FS)
 void amdgpu_sa_bo_dump_debug_info(struct amdgpu_sa_manager *sa_manager,
 					 struct seq_file *m);
diff --git a/amd/amdgpu/amdgpu_ring.h b/amd/amdgpu/amdgpu_ring.h
index 92bc89b..85179b0 100644
--- a/amd/amdgpu/amdgpu_ring.h
+++ b/amd/amdgpu/amdgpu_ring.h
@@ -68,7 +68,11 @@ struct amdgpu_fence_driver {
 	struct timer_list		fallback_timer;
 	unsigned			num_fences_mask;
 	spinlock_t			lock;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence		**fences;
+#else
 	struct fence			**fences;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 };
 
 int amdgpu_fence_driver_init(struct amdgpu_device *adev);
@@ -82,7 +86,11 @@ int amdgpu_fence_driver_start_ring(struct amdgpu_ring *ring,
 				   unsigned irq_type);
 void amdgpu_fence_driver_suspend(struct amdgpu_device *adev);
 void amdgpu_fence_driver_resume(struct amdgpu_device *adev);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+int amdgpu_fence_emit(struct amdgpu_ring *ring, struct dma_fence **fence);
+#else
 int amdgpu_fence_emit(struct amdgpu_ring *ring, struct fence **fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 void amdgpu_fence_process(struct amdgpu_ring *ring);
 int amdgpu_fence_wait_empty(struct amdgpu_ring *ring);
 unsigned amdgpu_fence_count_emitted(struct amdgpu_ring *ring);
diff --git a/amd/amdgpu/amdgpu_sa.c b/amd/amdgpu/amdgpu_sa.c
index 2778cfe..ed4dcb6 100644
--- a/amd/amdgpu/amdgpu_sa.c
+++ b/amd/amdgpu/amdgpu_sa.c
@@ -147,7 +147,11 @@ static void amdgpu_sa_bo_remove_locked(struct amdgpu_sa_bo *sa_bo)
 	}
 	list_del_init(&sa_bo->olist);
 	list_del_init(&sa_bo->flist);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(sa_bo->fence);
+#else
 	fence_put(sa_bo->fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	kfree(sa_bo);
 }
 
@@ -160,9 +164,15 @@ static void amdgpu_sa_bo_try_free(struct amdgpu_sa_manager *sa_manager)
 
 	sa_bo = list_entry(sa_manager->hole->next, struct amdgpu_sa_bo, olist);
 	list_for_each_entry_safe_from(sa_bo, tmp, &sa_manager->olist, olist) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		if (sa_bo->fence == NULL ||
+		    !dma_fence_is_signaled(sa_bo->fence)) {
+			return;
+#else
 		if (sa_bo->fence == NULL ||
 		    !fence_is_signaled(sa_bo->fence)) {
 			return;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		}
 		amdgpu_sa_bo_remove_locked(sa_bo);
 	}
@@ -243,9 +253,15 @@ static bool amdgpu_sa_event(struct amdgpu_sa_manager *sa_manager,
 	return false;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static bool amdgpu_sa_bo_next_hole(struct amdgpu_sa_manager *sa_manager,
+				   struct dma_fence **fences,
+				   unsigned *tries)
+#else
 static bool amdgpu_sa_bo_next_hole(struct amdgpu_sa_manager *sa_manager,
 				   struct fence **fences,
 				   unsigned *tries)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_sa_bo *best_bo = NULL;
 	unsigned i, soffset, best, tmp;
@@ -272,7 +288,11 @@ static bool amdgpu_sa_bo_next_hole(struct amdgpu_sa_manager *sa_manager,
 		sa_bo = list_first_entry(&sa_manager->flist[i],
 					 struct amdgpu_sa_bo, flist);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		if (!dma_fence_is_signaled(sa_bo->fence)) {
+#else
 		if (!fence_is_signaled(sa_bo->fence)) {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			fences[i] = sa_bo->fence;
 			continue;
 		}
@@ -314,7 +334,11 @@ int amdgpu_sa_bo_new(struct amdgpu_sa_manager *sa_manager,
 		     struct amdgpu_sa_bo **sa_bo,
 		     unsigned size, unsigned align)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fences[AMDGPU_SA_NUM_FENCE_LISTS];
+#else
 	struct fence *fences[AMDGPU_SA_NUM_FENCE_LISTS];
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned tries[AMDGPU_SA_NUM_FENCE_LISTS];
 	unsigned count;
 	int i, r;
@@ -355,14 +379,22 @@ int amdgpu_sa_bo_new(struct amdgpu_sa_manager *sa_manager,
 
 		for (i = 0, count = 0; i < AMDGPU_SA_NUM_FENCE_LISTS; ++i)
 			if (fences[i])
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				fences[count++] = dma_fence_get(fences[i]);
+#else
 				fences[count++] = fence_get(fences[i]);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		if (count) {
 			spin_unlock(&sa_manager->wq.lock);
 			t = kcl_fence_wait_any_timeout(fences, count, false,
 						   MAX_SCHEDULE_TIMEOUT, NULL);
 			for (i = 0; i < count; ++i)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				dma_fence_put(fences[i]);
+#else
 				fence_put(fences[i]);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 			r = (t > 0) ? 0 : t;
 			spin_lock(&sa_manager->wq.lock);
@@ -382,8 +414,13 @@ int amdgpu_sa_bo_new(struct amdgpu_sa_manager *sa_manager,
 	return r;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+void amdgpu_sa_bo_free(struct amdgpu_device *adev, struct amdgpu_sa_bo **sa_bo,
+		       struct dma_fence *fence)
+#else
 void amdgpu_sa_bo_free(struct amdgpu_device *adev, struct amdgpu_sa_bo **sa_bo,
 		       struct fence *fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_sa_manager *sa_manager;
 
@@ -393,10 +430,18 @@ void amdgpu_sa_bo_free(struct amdgpu_device *adev, struct amdgpu_sa_bo **sa_bo,
 
 	sa_manager = (*sa_bo)->manager;
 	spin_lock(&sa_manager->wq.lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (fence && !dma_fence_is_signaled(fence)) {
+#else
 	if (fence && !fence_is_signaled(fence)) {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		uint32_t idx;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		(*sa_bo)->fence = dma_fence_get(fence);
+#else
 		(*sa_bo)->fence = fence_get(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		idx = fence->context % AMDGPU_SA_NUM_FENCE_LISTS;
 		list_add_tail(&(*sa_bo)->flist, &sa_manager->flist[idx]);
 	} else {
diff --git a/amd/amdgpu/amdgpu_sem.c b/amd/amdgpu/amdgpu_sem.c
index db16baa..abd40ac 100644
--- a/amd/amdgpu/amdgpu_sem.c
+++ b/amd/amdgpu/amdgpu_sem.c
@@ -42,7 +42,11 @@ static int amdgpu_sem_cring_add(struct amdgpu_fpriv *fpriv,
 
 static const struct file_operations amdgpu_sem_fops;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static struct amdgpu_sem *amdgpu_sem_alloc(struct dma_fence *fence)
+#else
 static struct amdgpu_sem *amdgpu_sem_alloc(struct fence *fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_sem *sem;
 
@@ -72,7 +76,11 @@ static void amdgpu_sem_free(struct kref *kref)
 	struct amdgpu_sem *sem = container_of(
 		kref, struct amdgpu_sem, kref);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(sem->fence);
+#else
 	fence_put(sem->fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	kfree(sem);
 }
 
@@ -107,7 +115,11 @@ static int amdgpu_sem_create(void)
 	return get_unused_fd_flags(O_CLOEXEC);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static int amdgpu_sem_signal(int fd, struct dma_fence *fence)
+#else
 static int amdgpu_sem_signal(int fd, struct fence *fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_sem *sem;
 
@@ -147,12 +159,21 @@ static void amdgpu_sem_destroy(void)
 	 */
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static struct dma_fence *amdgpu_sem_get_fence(struct amdgpu_fpriv *fpriv,
+					      struct drm_amdgpu_sem_in *in)
+#else
 static struct fence *amdgpu_sem_get_fence(struct amdgpu_fpriv *fpriv,
 					 struct drm_amdgpu_sem_in *in)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_ring *out_ring;
 	struct amdgpu_ctx *ctx;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence;
+#else
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	uint32_t ctx_id, ip_type, ip_instance, ring;
 	int r;
 
@@ -220,7 +241,11 @@ int amdgpu_sem_add_cs(struct amdgpu_ctx *ctx, struct amdgpu_ring *ring,
 	list_for_each_entry_safe(sem, tmp, &ctx->rings[ring->idx].sem_list,
 				 list) {
 		r = amdgpu_sync_fence(ctx->adev, sync, sem->fence);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(sem->fence);
+#else
 		fence_put(sem->fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (r)
 			goto err;
 		list_del(&sem->list);
@@ -236,7 +261,11 @@ int amdgpu_sem_ioctl(struct drm_device *dev, void *data,
 {
 	union drm_amdgpu_sem *args = data;
 	struct amdgpu_fpriv *fpriv = filp->driver_priv;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence;
+#else
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int r = 0;
 	int fd = args->in.fd;
 
@@ -254,7 +283,11 @@ int amdgpu_sem_ioctl(struct drm_device *dev, void *data,
 			return r;
 		}
 		r = amdgpu_sem_signal(fd, fence);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		break;
 	case AMDGPU_SEM_OP_DESTROY_SEM:
 		amdgpu_sem_destroy();
diff --git a/amd/amdgpu/amdgpu_sem.h b/amd/amdgpu/amdgpu_sem.h
index 56d59d3..a0ba978 100644
--- a/amd/amdgpu/amdgpu_sem.h
+++ b/amd/amdgpu/amdgpu_sem.h
@@ -32,12 +32,20 @@
 #include <linux/ktime.h>
 #include <linux/list.h>
 #include <linux/spinlock.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+#include <linux/dma-fence.h>
+#else
 #include <linux/fence.h>
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 struct amdgpu_sem {
 	struct file		*file;
 	struct kref		kref;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence        *fence;
+#else
 	struct fence            *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct list_head        list;
 };
 
diff --git a/amd/amdgpu/amdgpu_sync.c b/amd/amdgpu/amdgpu_sync.c
index 10a144d..e5be739 100644
--- a/amd/amdgpu/amdgpu_sync.c
+++ b/amd/amdgpu/amdgpu_sync.c
@@ -34,7 +34,11 @@
 
 struct amdgpu_sync_entry {
 	struct hlist_node	node;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	*fence;
+#else
 	struct fence		*fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 };
 
 static struct kmem_cache *amdgpu_sync_slab;
@@ -60,7 +64,11 @@ void amdgpu_sync_create(struct amdgpu_sync *sync)
  *
  * Test if the fence was issued by us.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static bool amdgpu_sync_same_dev(struct amdgpu_device *adev, struct dma_fence *f)
+#else
 static bool amdgpu_sync_same_dev(struct amdgpu_device *adev, struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amd_sched_fence *s_fence = to_amd_sched_fence(f);
 
@@ -81,7 +89,11 @@ static bool amdgpu_sync_same_dev(struct amdgpu_device *adev, struct fence *f)
  *
  * Extract who originally created the fence.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void *amdgpu_sync_get_owner(struct dma_fence *f)
+#else
 static void *amdgpu_sync_get_owner(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amd_sched_fence *s_fence = to_amd_sched_fence(f);
 
@@ -99,6 +111,16 @@ static void *amdgpu_sync_get_owner(struct fence *f)
  *
  * Either keep the existing fence or the new one, depending which one is later.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amdgpu_sync_keep_later(struct dma_fence **keep, struct dma_fence *fence)
+{
+	if (*keep && dma_fence_is_later(*keep, fence))
+		return;
+
+	dma_fence_put(*keep);
+	*keep = dma_fence_get(fence);
+}
+#else
 static void amdgpu_sync_keep_later(struct fence **keep, struct fence *fence)
 {
 	if (*keep && fence_is_later(*keep, fence))
@@ -107,6 +129,7 @@ static void amdgpu_sync_keep_later(struct fence **keep, struct fence *fence)
 	fence_put(*keep);
 	*keep = fence_get(fence);
 }
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 /**
  * amdgpu_sync_add_later - add the fence to the hash
@@ -117,7 +140,11 @@ static void amdgpu_sync_keep_later(struct fence **keep, struct fence *fence)
  * Tries to add the fence to an existing hash entry. Returns true when an entry
  * was found, false otherwise.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static bool amdgpu_sync_add_later(struct amdgpu_sync *sync, struct dma_fence *f)
+#else
 static bool amdgpu_sync_add_later(struct amdgpu_sync *sync, struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_sync_entry *e;
 
@@ -145,7 +172,11 @@ static bool amdgpu_sync_add_later(struct amdgpu_sync *sync, struct fence *f)
  *
  */
 int amdgpu_sync_fence(struct amdgpu_device *adev, struct amdgpu_sync *sync,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		      struct dma_fence *f)
+#else
 		      struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_sync_entry *e;
 
@@ -164,7 +195,11 @@ int amdgpu_sync_fence(struct amdgpu_device *adev, struct amdgpu_sync *sync,
 		return -ENOMEM;
 
 	hash_add(sync->fences, &e->node, f->context);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	e->fence = dma_fence_get(f);
+#else
 	e->fence = fence_get(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return 0;
 }
 
@@ -183,7 +218,11 @@ int amdgpu_sync_resv(struct amdgpu_device *adev,
 		     void *owner)
 {
 	struct reservation_object_list *flist;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f;
+#else
 	struct fence *f;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	void *fence_owner;
 	unsigned i;
 	int r = 0;
@@ -237,8 +276,13 @@ int amdgpu_sync_resv(struct amdgpu_device *adev,
  * Returns the next fence not signaled yet without removing it from the sync
  * object.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+struct dma_fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,
+				     struct amdgpu_ring *ring)
+#else
 struct fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,
 				     struct amdgpu_ring *ring)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_sync_entry *e;
 	struct hlist_node *tmp;
@@ -251,7 +295,11 @@ struct fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,
 #else
 	hash_for_each_safe(sync->fences, i, tmp, e, node) {
 #endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *f = e->fence;
+#else
 		struct fence *f = e->fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		struct amd_sched_fence *s_fence = to_amd_sched_fence(f);
 
 		if (ring && s_fence) {
@@ -259,19 +307,33 @@ struct fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,
 			 * when they are scheduled.
 			 */
 			if (s_fence->sched == &ring->sched) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				if (dma_fence_is_signaled(&s_fence->scheduled))
+					continue;
+#else
 				if (fence_is_signaled(&s_fence->scheduled))
 					continue;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 				return &s_fence->scheduled;
 			}
 		}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		if (dma_fence_is_signaled(f)) {
+			hash_del(&e->node);
+			dma_fence_put(f);
+			kmem_cache_free(amdgpu_sync_slab, e);
+			continue;
+		}
+#else
 		if (fence_is_signaled(f)) {
 			hash_del(&e->node);
 			fence_put(f);
 			kmem_cache_free(amdgpu_sync_slab, e);
 			continue;
 		}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		return f;
 	}
@@ -286,11 +348,19 @@ struct fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,
  *
  * Get and removes the next fence from the sync object not signaled yet.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+struct dma_fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync)
+#else
 struct fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_sync_entry *e;
 	struct hlist_node *tmp;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f;
+#else
 	struct fence *f;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int i;
 #if LINUX_VERSION_CODE < KERNEL_VERSION(3, 9, 0)
 	struct hlist_node *node;
@@ -305,10 +375,17 @@ struct fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync)
 		hash_del(&e->node);
 		kmem_cache_free(amdgpu_sync_slab, e);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		if (!dma_fence_is_signaled(f))
+			return f;
+
+		dma_fence_put(f);
+#else
 		if (!fence_is_signaled(f))
 			return f;
 
 		fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	}
 	return NULL;
 }
@@ -325,12 +402,20 @@ int amdgpu_sync_wait(struct amdgpu_sync *sync)
 #else
 	hash_for_each_safe(sync->fences, i, tmp, e, node) {
 #endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		r = dma_fence_wait(e->fence, false);
+#else
 		r = fence_wait(e->fence, false);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (r)
 			return r;
 
 		hash_del(&e->node);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(e->fence);
+#else
 		fence_put(e->fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		kmem_cache_free(amdgpu_sync_slab, e);
 	}
 
@@ -357,11 +442,19 @@ void amdgpu_sync_free(struct amdgpu_sync *sync)
 	hash_for_each_safe(sync->fences, i, tmp, e, node) {
 #endif
 		hash_del(&e->node);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(e->fence);
+#else
 		fence_put(e->fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		kmem_cache_free(amdgpu_sync_slab, e);
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(sync->last_vm_update);
+#else
 	fence_put(sync->last_vm_update);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 }
 
 /**
diff --git a/amd/amdgpu/amdgpu_sync.h b/amd/amdgpu/amdgpu_sync.h
index 405f379..78c6a98 100644
--- a/amd/amdgpu/amdgpu_sync.h
+++ b/amd/amdgpu/amdgpu_sync.h
@@ -26,7 +26,11 @@
 
 #include <linux/hashtable.h>
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+struct dma_fence;
+#else
 struct fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 struct reservation_object;
 struct amdgpu_device;
 struct amdgpu_ring;
@@ -36,19 +40,33 @@ struct amdgpu_ring;
  */
 struct amdgpu_sync {
 	DECLARE_HASHTABLE(fences, 4);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	*last_vm_update;
+#else
 	struct fence	*last_vm_update;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 };
 
 void amdgpu_sync_create(struct amdgpu_sync *sync);
 int amdgpu_sync_fence(struct amdgpu_device *adev, struct amdgpu_sync *sync,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		      struct dma_fence *f);
+#else
 		      struct fence *f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 int amdgpu_sync_resv(struct amdgpu_device *adev,
 		     struct amdgpu_sync *sync,
 		     struct reservation_object *resv,
 		     void *owner);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+struct dma_fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,
+					 struct amdgpu_ring *ring);
+struct dma_fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync);
+#else
 struct fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,
 				     struct amdgpu_ring *ring);
 struct fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 void amdgpu_sync_free(struct amdgpu_sync *sync);
 int amdgpu_sync_init(void);
 void amdgpu_sync_fini(void);
diff --git a/amd/amdgpu/amdgpu_test.c b/amd/amdgpu/amdgpu_test.c
index b827c75..0230179 100644
--- a/amd/amdgpu/amdgpu_test.c
+++ b/amd/amdgpu/amdgpu_test.c
@@ -78,7 +78,11 @@ static void amdgpu_do_test_moves(struct amdgpu_device *adev)
 		void *gtt_map, *vram_map;
 		void **gtt_start, **gtt_end;
 		void **vram_start, **vram_end;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence = NULL;
+#else
 		struct fence *fence = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		r = amdgpu_bo_create(adev, size, PAGE_SIZE, true,
 				     AMDGPU_GEM_DOMAIN_GTT, 0, NULL,
@@ -118,13 +122,21 @@ static void amdgpu_do_test_moves(struct amdgpu_device *adev)
 			goto out_lclean_unpin;
 		}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		r = dma_fence_wait(fence, false);
+#else
 		r = fence_wait(fence, false);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (r) {
 			DRM_ERROR("Failed to wait for GTT->VRAM fence %d\n", i);
 			goto out_lclean_unpin;
 		}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		r = amdgpu_bo_kmap(vram_obj, &vram_map);
 		if (r) {
@@ -163,13 +175,21 @@ static void amdgpu_do_test_moves(struct amdgpu_device *adev)
 			goto out_lclean_unpin;
 		}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		r = dma_fence_wait(fence, false);
+#else
 		r = fence_wait(fence, false);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (r) {
 			DRM_ERROR("Failed to wait for VRAM->GTT fence %d\n", i);
 			goto out_lclean_unpin;
 		}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		r = amdgpu_bo_kmap(gtt_obj[i], &gtt_map);
 		if (r) {
@@ -216,7 +236,11 @@ out_lclean:
 			amdgpu_bo_unref(&gtt_obj[i]);
 		}
 		if (fence)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_put(fence);
+#else
 			fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		break;
 	}
 
diff --git a/amd/amdgpu/amdgpu_trace.h b/amd/amdgpu/amdgpu_trace.h
index 067e5e6..df4e2ef 100644
--- a/amd/amdgpu/amdgpu_trace.h
+++ b/amd/amdgpu/amdgpu_trace.h
@@ -104,7 +104,11 @@ TRACE_EVENT(amdgpu_cs_ioctl,
 			     __field(struct amdgpu_device *, adev)
 			     __field(struct amd_sched_job *, sched_job)
 			     __field(struct amdgpu_ib *, ib)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			     __field(struct dma_fence *, fence)
+#else
 			     __field(struct fence *, fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			     __field(char *, ring_name)
 			     __field(u32, num_ibs)
 			     ),
@@ -129,7 +133,11 @@ TRACE_EVENT(amdgpu_sched_run_job,
 			     __field(struct amdgpu_device *, adev)
 			     __field(struct amd_sched_job *, sched_job)
 			     __field(struct amdgpu_ib *, ib)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			     __field(struct dma_fence *, fence)
+#else
 			     __field(struct fence *, fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			     __field(char *, ring_name)
 			     __field(u32, num_ibs)
 			     ),
diff --git a/amd/amdgpu/amdgpu_ttm.c b/amd/amdgpu/amdgpu_ttm.c
index fa8998c..143b149 100644
--- a/amd/amdgpu/amdgpu_ttm.c
+++ b/amd/amdgpu/amdgpu_ttm.c
@@ -310,7 +310,11 @@ static int amdgpu_move_blit(struct ttm_buffer_object *bo,
 	struct drm_mm_node *old_mm, *new_mm;
 	uint64_t old_start, old_size, new_start, new_size;
 	unsigned long num_pages;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence = NULL;
+#else
 	struct fence *fence = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int r;
 
 	BUILD_BUG_ON((PAGE_SIZE % AMDGPU_GPU_PAGE_SIZE) != 0);
@@ -336,7 +340,11 @@ static int amdgpu_move_blit(struct ttm_buffer_object *bo,
 	num_pages = new_mem->num_pages;
 	while (num_pages) {
 		unsigned long cur_pages = min(old_size, new_size);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *next;
+#else
 		struct fence *next;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		r = amdgpu_copy_buffer(ring, old_start, new_start,
 				       cur_pages * PAGE_SIZE,
@@ -344,7 +352,11 @@ static int amdgpu_move_blit(struct ttm_buffer_object *bo,
 		if (r)
 			goto error;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		fence = next;
 
 		num_pages -= cur_pages;
@@ -376,13 +388,23 @@ static int amdgpu_move_blit(struct ttm_buffer_object *bo,
 	}
 
 	r = ttm_bo_pipeline_move(bo, fence, evict, new_mem);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(fence);
+#else
 	fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return r;
 
 error:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (fence)
+		dma_fence_wait(fence, false);
+	dma_fence_put(fence);
+#else
 	if (fence)
 		fence_wait(fence, false);
 	fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return r;
 }
 
@@ -1427,7 +1449,12 @@ int amdgpu_copy_buffer(struct amdgpu_ring *ring,
 		       uint64_t dst_offset,
 		       uint32_t byte_count,
 		       struct reservation_object *resv,
-		       struct fence **fence, bool direct_submit)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		       struct dma_fence **fence,
+#else
+		       struct fence **fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
+		       bool direct_submit)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_job *job;
@@ -1474,7 +1501,11 @@ int amdgpu_copy_buffer(struct amdgpu_ring *ring,
 	if (direct_submit) {
 		r = amdgpu_ib_schedule(ring, job->num_ibs, job->ibs,
 				       NULL, NULL, fence);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		job->fence = dma_fence_get(*fence);
+#else
 		job->fence = fence_get(*fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (r)
 			DRM_ERROR("Error scheduling IBs (%d)\n", r);
 		amdgpu_job_free(job);
@@ -1494,7 +1525,11 @@ error_free:
 
 int amdgpu_fill_buffer(struct amdgpu_bo *bo, uint32_t src_data,
 		       struct reservation_object *resv,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		       struct dma_fence **fence)
+#else
 		       struct fence **fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);
 	uint32_t max_bytes = adev->mman.buffer_funcs->fill_max_bytes;
diff --git a/amd/amdgpu/amdgpu_ttm.h b/amd/amdgpu/amdgpu_ttm.h
index 4926bc7..8057f09 100644
--- a/amd/amdgpu/amdgpu_ttm.h
+++ b/amd/amdgpu/amdgpu_ttm.h
@@ -82,11 +82,20 @@ int amdgpu_copy_buffer(struct amdgpu_ring *ring,
 		       uint64_t dst_offset,
 		       uint32_t byte_count,
 		       struct reservation_object *resv,
-		       struct fence **fence, bool direct_submit);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		       struct dma_fence **fence,
+#else
+		       struct fence **fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
+		       bool direct_submit);
 int amdgpu_fill_buffer(struct amdgpu_bo *bo,
 			uint32_t src_data,
 			struct reservation_object *resv,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			struct dma_fence **fence);
+#else
 			struct fence **fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 int amdgpu_mmap(struct file *filp, struct vm_area_struct *vma);
 bool amdgpu_ttm_is_bound(struct ttm_tt *ttm);
diff --git a/amd/amdgpu/amdgpu_uvd.c b/amd/amdgpu/amdgpu_uvd.c
index a8816ba..285ad88 100644
--- a/amd/amdgpu/amdgpu_uvd.c
+++ b/amd/amdgpu/amdgpu_uvd.c
@@ -333,7 +333,11 @@ void amdgpu_uvd_free_handles(struct amdgpu_device *adev, struct drm_file *filp)
 	for (i = 0; i < adev->uvd.max_handles; ++i) {
 		uint32_t handle = atomic_read(&adev->uvd.handles[i]);
 		if (handle != 0 && adev->uvd.filp[i] == filp) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			struct dma_fence *fence;
+#else
 			struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 			r = amdgpu_uvd_get_destroy_msg(ring, handle,
 						       false, &fence);
@@ -342,8 +346,13 @@ void amdgpu_uvd_free_handles(struct amdgpu_device *adev, struct drm_file *filp)
 				continue;
 			}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_wait(fence, false);
+			dma_fence_put(fence);
+#else
 			fence_wait(fence, false);
 			fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 			adev->uvd.filp[i] = NULL;
 			atomic_set(&adev->uvd.handles[i], 0);
@@ -920,15 +929,24 @@ int amdgpu_uvd_ring_parse_cs(struct amdgpu_cs_parser *parser, uint32_t ib_idx)
 	return 0;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,
+			       bool direct, struct dma_fence **fence)
+#else
 static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,
 			       bool direct, struct fence **fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct ttm_validate_buffer tv;
 	struct ww_acquire_ctx ticket;
 	struct list_head head;
 	struct amdgpu_job *job;
 	struct amdgpu_ib *ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct amdgpu_device *adev = ring->adev;
 	uint64_t addr;
 	int i, r;
@@ -972,7 +990,11 @@ static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,
 
 	if (direct) {
 		r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		job->fence = dma_fence_get(f);
+#else
 		job->fence = fence_get(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (r)
 			goto err_free;
 
@@ -986,10 +1008,17 @@ static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,
 
 	ttm_eu_fence_buffer_objects(&ticket, &head, f);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (fence)
+		*fence = dma_fence_get(f);
+	amdgpu_bo_unref(&bo);
+	dma_fence_put(f);
+#else
 	if (fence)
 		*fence = fence_get(f);
 	amdgpu_bo_unref(&bo);
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	return 0;
 
@@ -1004,8 +1033,13 @@ err:
 /* multiple fence commands without any stream commands in between can
    crash the vcpu so just try to emmit a dummy create/destroy msg to
    avoid this */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+int amdgpu_uvd_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
+			      struct dma_fence **fence)
+#else
 int amdgpu_uvd_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 			      struct fence **fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_bo *bo;
@@ -1054,8 +1088,13 @@ int amdgpu_uvd_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 	return amdgpu_uvd_send_msg(ring, bo, true, fence);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+int amdgpu_uvd_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
+			       bool direct, struct dma_fence **fence)
+#else
 int amdgpu_uvd_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
 			       bool direct, struct fence **fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_bo *bo;
@@ -1142,7 +1181,11 @@ void amdgpu_uvd_ring_end_use(struct amdgpu_ring *ring)
  */
 int amdgpu_uvd_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence;
+#else
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	long r;
 
 	r = amdgpu_uvd_get_create_msg(ring, 1, NULL);
@@ -1157,7 +1200,11 @@ int amdgpu_uvd_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 		goto error;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait_timeout(fence, false, timeout);
+#else
 	r = fence_wait_timeout(fence, false, timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r == 0) {
 		DRM_ERROR("amdgpu: IB test timed out.\n");
 		r = -ETIMEDOUT;
@@ -1168,7 +1215,11 @@ int amdgpu_uvd_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 		r = 0;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(fence);
+#else
 	fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 error:
 	return r;
diff --git a/amd/amdgpu/amdgpu_uvd.h b/amd/amdgpu/amdgpu_uvd.h
index c850009..7124a52 100644
--- a/amd/amdgpu/amdgpu_uvd.h
+++ b/amd/amdgpu/amdgpu_uvd.h
@@ -29,9 +29,18 @@ int amdgpu_uvd_sw_fini(struct amdgpu_device *adev);
 int amdgpu_uvd_suspend(struct amdgpu_device *adev);
 int amdgpu_uvd_resume(struct amdgpu_device *adev);
 int amdgpu_uvd_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			      struct dma_fence **fence);
+#else
 			      struct fence **fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 int amdgpu_uvd_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
-			       bool direct, struct fence **fence);
+			       bool direct,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			       struct dma_fence **fence);
+#else
+			       struct fence **fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 void amdgpu_uvd_free_handles(struct amdgpu_device *adev,
 			     struct drm_file *filp);
 int amdgpu_uvd_ring_parse_cs(struct amdgpu_cs_parser *parser, uint32_t ib_idx);
diff --git a/amd/amdgpu/amdgpu_vce.c b/amd/amdgpu/amdgpu_vce.c
index 3d6f86c..d0c69e1 100644
--- a/amd/amdgpu/amdgpu_vce.c
+++ b/amd/amdgpu/amdgpu_vce.c
@@ -395,13 +395,22 @@ void amdgpu_vce_free_handles(struct amdgpu_device *adev, struct drm_file *filp)
  *
  * Open up a stream for HW test
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
+			      struct dma_fence **fence)
+#else
 int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 			      struct fence **fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	const unsigned ib_size_dw = 1024;
 	struct amdgpu_job *job;
 	struct amdgpu_ib *ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	uint64_t dummy;
 	int i, r;
 
@@ -451,14 +460,24 @@ int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
 		ib->ptr[i] = 0x0;
 
 	r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	job->fence = dma_fence_get(f);
+#else
 	job->fence = fence_get(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r)
 		goto err;
 
 	amdgpu_job_free(job);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (fence)
+		*fence = dma_fence_get(f);
+	dma_fence_put(f);
+#else
 	if (fence)
 		*fence = fence_get(f);
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return 0;
 
 err:
@@ -476,13 +495,22 @@ err:
  *
  * Close up a stream for HW test or if userspace failed to do so
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
+			       bool direct, struct dma_fence **fence)
+#else
 int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
 			       bool direct, struct fence **fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	const unsigned ib_size_dw = 1024;
 	struct amdgpu_job *job;
 	struct amdgpu_ib *ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int i, r;
 
 	r = amdgpu_job_alloc_with_ib(ring->adev, ib_size_dw * 4, &job);
@@ -514,7 +542,11 @@ int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
 
 	if (direct) {
 		r = amdgpu_ib_schedule(ring, 1, ib, NULL, NULL, &f);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		job->fence = dma_fence_get(f);
+#else
 		job->fence = fence_get(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (r)
 			goto err;
 
@@ -526,9 +558,15 @@ int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
 			goto err;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (fence)
+		*fence = dma_fence_get(f);
+	dma_fence_put(f);
+#else
 	if (fence)
 		*fence = fence_get(f);
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return 0;
 
 err:
@@ -965,7 +1003,11 @@ int amdgpu_vce_ring_test_ring(struct amdgpu_ring *ring)
  */
 int amdgpu_vce_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence = NULL;
+#else
 	struct fence *fence = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	long r;
 
 	/* skip vce ring1/2 ib test for now, since it's not reliable */
@@ -984,7 +1026,11 @@ int amdgpu_vce_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 		goto error;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait_timeout(fence, false, timeout);
+#else
 	r = fence_wait_timeout(fence, false, timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r == 0) {
 		DRM_ERROR("amdgpu: IB test timed out.\n");
 		r = -ETIMEDOUT;
@@ -995,6 +1041,10 @@ int amdgpu_vce_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 		r = 0;
 	}
 error:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(fence);
+#else
 	fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return r;
 }
diff --git a/amd/amdgpu/amdgpu_vce.h b/amd/amdgpu/amdgpu_vce.h
index 44d49b5..88c7412 100644
--- a/amd/amdgpu/amdgpu_vce.h
+++ b/amd/amdgpu/amdgpu_vce.h
@@ -29,9 +29,18 @@ int amdgpu_vce_sw_fini(struct amdgpu_device *adev);
 int amdgpu_vce_suspend(struct amdgpu_device *adev);
 int amdgpu_vce_resume(struct amdgpu_device *adev);
 int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			      struct dma_fence **fence);
+#else
 			      struct fence **fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,
-			       bool direct, struct fence **fence);
+			       bool direct,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			       struct dma_fence **fence);
+#else
+			       struct fence **fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 void amdgpu_vce_free_handles(struct amdgpu_device *adev, struct drm_file *filp);
 int amdgpu_vce_ring_parse_cs(struct amdgpu_cs_parser *p, uint32_t ib_idx);
 int amdgpu_vce_ring_parse_cs_vm(struct amdgpu_cs_parser *p, uint32_t ib_idx);
diff --git a/amd/amdgpu/amdgpu_vm.c b/amd/amdgpu/amdgpu_vm.c
index 945948b..1e7e32d 100644
--- a/amd/amdgpu/amdgpu_vm.c
+++ b/amd/amdgpu/amdgpu_vm.c
@@ -202,15 +202,26 @@ static bool amdgpu_vm_is_gpu_reset(struct amdgpu_device *adev,
  *
  * Allocate an id for the vm, adding fences to the sync obj as necessary.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
+		      struct amdgpu_sync *sync, struct dma_fence *fence,
+		      struct amdgpu_job *job)
+#else
 int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 		      struct amdgpu_sync *sync, struct fence *fence,
 		      struct amdgpu_job *job)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_device *adev = ring->adev;
 	uint64_t fence_context = adev->fence_context + ring->idx;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *updates = sync->last_vm_update;
+	struct dma_fence **fences;
+#else
 	struct fence *updates = sync->last_vm_update;
-	struct amdgpu_vm_id *id, *idle;
 	struct fence **fences;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
+	struct amdgpu_vm_id *id, *idle;
 	unsigned i;
 	int r = 0;
 
@@ -234,17 +245,29 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 	if (&idle->list == &adev->vm_manager.ids_lru) {
 		u64 fence_context = adev->vm_manager.fence_context + ring->idx;
 		unsigned seqno = ++adev->vm_manager.seqno[ring->idx];
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence_array *array;
+#else
 		struct fence_array *array;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		unsigned j;
 
 		for (j = 0; j < i; ++j)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_get(fences[j]);
+#else
 			fence_get(fences[j]);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		array = fence_array_create(i, fences, fence_context,
 					   seqno, true);
 		if (!array) {
 			for (j = 0; j < i; ++j)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				dma_fence_put(fences[j]);
+#else
 				fence_put(fences[j]);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			kfree(fences);
 			r = -ENOMEM;
 			goto error;
@@ -252,7 +275,11 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 
 
 		r = amdgpu_sync_fence(ring->adev, sync, &array->base);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(&array->base);
+#else
 		fence_put(&array->base);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (r)
 			goto error;
 
@@ -266,7 +293,11 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 	/* Check if we can use a VMID already assigned to this VM */
 	i = ring->idx;
 	do {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *flushed;
+#else
 		struct fence *flushed;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		id = vm->ids[i++];
 		if (i == AMDGPU_MAX_RINGS)
@@ -287,6 +318,16 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 		if (!id->last_flush)
 			continue;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		if (id->last_flush->context != fence_context &&
+		    !dma_fence_is_signaled(id->last_flush))
+			continue;
+
+		flushed  = id->flushed_updates;
+		if (updates &&
+		    (!flushed || dma_fence_is_later(updates, flushed)))
+			continue;
+#else
 		if (id->last_flush->context != fence_context &&
 		    !fence_is_signaled(id->last_flush))
 			continue;
@@ -295,6 +336,7 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 		if (updates &&
 		    (!flushed || fence_is_later(updates, flushed)))
 			continue;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		/* Good we can use this VMID. Remember this submission as
 		 * user of the VMID.
@@ -324,6 +366,16 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 	if (r)
 		goto error;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(id->first);
+	id->first = dma_fence_get(fence);
+
+	dma_fence_put(id->last_flush);
+	id->last_flush = NULL;
+
+	dma_fence_put(id->flushed_updates);
+	id->flushed_updates = dma_fence_get(updates);
+#else
 	fence_put(id->first);
 	id->first = fence_get(fence);
 
@@ -332,6 +384,7 @@ int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
 
 	fence_put(id->flushed_updates);
 	id->flushed_updates = fence_get(updates);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	id->pd_gpu_addr = job->vm_pd_addr;
 	id->current_gpu_reset_count = atomic_read(&adev->gpu_reset_counter);
@@ -402,7 +455,11 @@ int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job)
 
 	if (ring->funcs->emit_vm_flush && (job->vm_needs_flush ||
 	    amdgpu_vm_is_gpu_reset(adev, id))) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence;
+#else
 		struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		trace_amdgpu_vm_flush(job->vm_pd_addr, ring->idx, job->vm_id);
 		amdgpu_ring_emit_vm_flush(ring, job->vm_id, job->vm_pd_addr);
@@ -412,7 +469,11 @@ int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job)
 			return r;
 
 		mutex_lock(&adev->vm_manager.lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(id->last_flush);
+#else
 		fence_put(id->last_flush);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		id->last_flush = fence;
 		mutex_unlock(&adev->vm_manager.lock);
 	}
@@ -580,7 +641,11 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 	unsigned count = 0, pt_idx, ndw;
 	struct amdgpu_job *job;
 	struct amdgpu_pte_update_params params;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence = NULL;
+#else
 	struct fence *fence = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	int r;
 
@@ -690,9 +755,15 @@ int amdgpu_vm_update_page_directory(struct amdgpu_device *adev,
 		goto error_free;
 
 	amdgpu_bo_fence(vm->page_directory, fence, true);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(vm->page_directory_fence);
+	vm->page_directory_fence = dma_fence_get(fence);
+	dma_fence_put(fence);
+#else
 	fence_put(vm->page_directory_fence);
 	vm->page_directory_fence = fence_get(fence);
 	fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	return 0;
 
@@ -877,6 +948,16 @@ static void amdgpu_vm_frag_ptes(struct amdgpu_pte_update_params	*params,
  * Fill in the page table entries between @start and @last.
  * Returns 0 for success, -EINVAL for failure.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
+				       struct dma_fence *exclusive,
+				       uint64_t src,
+				       dma_addr_t *pages_addr,
+				       struct amdgpu_vm *vm,
+				       uint64_t start, uint64_t last,
+				       uint32_t flags, uint64_t addr,
+				       struct dma_fence **fence)
+#else
 static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 				       struct fence *exclusive,
 				       uint64_t src,
@@ -885,13 +966,18 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 				       uint64_t start, uint64_t last,
 				       uint32_t flags, uint64_t addr,
 				       struct fence **fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amdgpu_ring *ring;
 	void *owner = AMDGPU_FENCE_OWNER_VM;
 	unsigned nptes, ncmds, ndw;
 	struct amdgpu_job *job;
 	struct amdgpu_pte_update_params params;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int r;
 
 	memset(&params, 0, sizeof(params));
@@ -993,11 +1079,19 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
 		goto error_free;
 
 	amdgpu_bo_fence(vm->page_directory, f, true);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (fence) {
+		dma_fence_put(*fence);
+		*fence = dma_fence_get(f);
+	}
+	dma_fence_put(f);
+#else
 	if (fence) {
 		fence_put(*fence);
 		*fence = fence_get(f);
 	}
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return 0;
 
 error_free:
@@ -1022,6 +1116,17 @@ error_free:
  * into a SDMA IB.
  * Returns 0 for success, -EINVAL for failure.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static int amdgpu_vm_bo_split_mapping(struct amdgpu_device *adev,
+				      struct dma_fence *exclusive,
+				      uint32_t gtt_flags,
+				      dma_addr_t *pages_addr,
+				      struct amdgpu_vm *vm,
+				      struct amdgpu_bo_va_mapping *mapping,
+				      uint32_t flags,
+				      struct ttm_mem_reg *mem,
+				      struct dma_fence **fence)
+#else
 static int amdgpu_vm_bo_split_mapping(struct amdgpu_device *adev,
 				      struct fence *exclusive,
 				      uint32_t gtt_flags,
@@ -1031,6 +1136,7 @@ static int amdgpu_vm_bo_split_mapping(struct amdgpu_device *adev,
 				      uint32_t flags,
 				      struct ttm_mem_reg *mem,
 				      struct fence **fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct drm_mm_node *nodes = mem ? mem->mm_node : NULL;
 	uint64_t pfn, src = 0, start = mapping->it.start;
@@ -1134,7 +1240,11 @@ int amdgpu_vm_bo_update(struct amdgpu_device *adev,
 	uint32_t gtt_flags, flags;
 	struct ttm_mem_reg *mem;
 	struct drm_mm_node *nodes;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *exclusive;
+#else
 	struct fence *exclusive;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int r;
 
 	if (clear) {
@@ -1510,7 +1620,11 @@ void amdgpu_vm_bo_rmv(struct amdgpu_device *adev,
 		kfree(mapping);
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(bo_va->last_pt_update);
+#else
 	fence_put(bo_va->last_pt_update);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	kfree(bo_va);
 }
 
@@ -1661,7 +1775,11 @@ void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)
 
 	amdgpu_bo_unref(&vm->page_directory->shadow);
 	amdgpu_bo_unref(&vm->page_directory);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(vm->page_directory_fence);
+#else
 	fence_put(vm->page_directory_fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 }
 
 /**
@@ -1707,9 +1825,16 @@ void amdgpu_vm_manager_fini(struct amdgpu_device *adev)
 	for (i = 0; i < AMDGPU_NUM_VM; ++i) {
 		struct amdgpu_vm_id *id = &adev->vm_manager.ids[i];
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(adev->vm_manager.ids[i].first);
+		amdgpu_sync_free(&adev->vm_manager.ids[i].active);
+		dma_fence_put(id->flushed_updates);
+		dma_fence_put(id->last_flush);
+#else
 		fence_put(adev->vm_manager.ids[i].first);
 		amdgpu_sync_free(&adev->vm_manager.ids[i].active);
 		fence_put(id->flushed_updates);
 		fence_put(id->last_flush);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	}
 }
diff --git a/amd/amdgpu/amdgpu_vm.h b/amd/amdgpu/amdgpu_vm.h
index 3e89de4..212d9cb 100644
--- a/amd/amdgpu/amdgpu_vm.h
+++ b/amd/amdgpu/amdgpu_vm.h
@@ -94,7 +94,11 @@ struct amdgpu_vm {
 	/* contains the page directory */
 	struct amdgpu_bo	*page_directory;
 	unsigned		max_pde_used;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	*page_directory_fence;
+#else
 	struct fence		*page_directory_fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	uint64_t		last_eviction_counter;
 
 	/* array of page tables, one for each page directory entry */
@@ -115,14 +119,26 @@ struct amdgpu_vm {
 
 struct amdgpu_vm_id {
 	struct list_head	list;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	*first;
+#else
 	struct fence		*first;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct amdgpu_sync	active;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	*last_flush;
+#else
 	struct fence		*last_flush;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	atomic64_t		owner;
 
 	uint64_t		pd_gpu_addr;
 	/* last flushed PD/PT update */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence	*flushed_updates;
+#else
 	struct fence		*flushed_updates;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	uint32_t                current_gpu_reset_count;
 
@@ -177,7 +193,12 @@ int amdgpu_vm_validate_pt_bos(struct amdgpu_device *adev, struct amdgpu_vm *vm,
 void amdgpu_vm_move_pt_bos_in_lru(struct amdgpu_device *adev,
 				  struct amdgpu_vm *vm);
 int amdgpu_vm_grab_id(struct amdgpu_vm *vm, struct amdgpu_ring *ring,
-		      struct amdgpu_sync *sync, struct fence *fence,
+		      struct amdgpu_sync *sync,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		      struct dma_fence *fence,
+#else
+		      struct fence *fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		      struct amdgpu_job *job);
 int amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job);
 void amdgpu_vm_reset_id(struct amdgpu_device *adev, unsigned vm_id);
diff --git a/amd/amdgpu/cik_sdma.c b/amd/amdgpu/cik_sdma.c
index 914058f..fd7cd73 100644
--- a/amd/amdgpu/cik_sdma.c
+++ b/amd/amdgpu/cik_sdma.c
@@ -621,7 +621,11 @@ static int cik_sdma_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned index;
 	u32 tmp = 0;
 	u64 gpu_addr;
@@ -654,7 +658,11 @@ static int cik_sdma_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 	if (r)
 		goto err1;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait_timeout(f, false, timeout);
+#else
 	r = fence_wait_timeout(f, false, timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r == 0) {
 		DRM_ERROR("amdgpu: IB test timed out\n");
 		r = -ETIMEDOUT;
@@ -674,7 +682,11 @@ static int cik_sdma_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 
 err1:
 	amdgpu_ib_free(adev, &ib, NULL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 err0:
 	amdgpu_wb_free(adev, index);
 	return r;
diff --git a/amd/amdgpu/gfx_v6_0.c b/amd/amdgpu/gfx_v6_0.c
index 4a633a3..18995ba 100644
--- a/amd/amdgpu/gfx_v6_0.c
+++ b/amd/amdgpu/gfx_v6_0.c
@@ -1953,7 +1953,11 @@ static int gfx_v6_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	uint32_t scratch;
 	uint32_t tmp = 0;
 	long r;
@@ -1979,7 +1983,11 @@ static int gfx_v6_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 	if (r)
 		goto err2;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait_timeout(f, false, timeout);
+#else
 	r = fence_wait_timeout(f, false, timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r == 0) {
 		DRM_ERROR("amdgpu: IB test timed out\n");
 		r = -ETIMEDOUT;
@@ -2000,7 +2008,11 @@ static int gfx_v6_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 
 err2:
 	amdgpu_ib_free(adev, &ib, NULL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 err1:
 	amdgpu_gfx_scratch_free(adev, scratch);
 	return r;
diff --git a/amd/amdgpu/gfx_v7_0.c b/amd/amdgpu/gfx_v7_0.c
index acb91f1..a50cbf5 100644
--- a/amd/amdgpu/gfx_v7_0.c
+++ b/amd/amdgpu/gfx_v7_0.c
@@ -2317,7 +2317,11 @@ static int gfx_v7_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	uint32_t scratch;
 	uint32_t tmp = 0;
 	long r;
@@ -2343,7 +2347,11 @@ static int gfx_v7_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 	if (r)
 		goto err2;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait_timeout(f, false, timeout);
+#else
 	r = fence_wait_timeout(f, false, timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r == 0) {
 		DRM_ERROR("amdgpu: IB test timed out\n");
 		r = -ETIMEDOUT;
@@ -2364,7 +2372,11 @@ static int gfx_v7_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 
 err2:
 	amdgpu_ib_free(adev, &ib, NULL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 err1:
 	amdgpu_gfx_scratch_free(adev, scratch);
 	return r;
diff --git a/amd/amdgpu/gfx_v8_0.c b/amd/amdgpu/gfx_v8_0.c
index 6853969..7386eb5 100644
--- a/amd/amdgpu/gfx_v8_0.c
+++ b/amd/amdgpu/gfx_v8_0.c
@@ -799,7 +799,11 @@ static int gfx_v8_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	uint32_t scratch;
 	uint32_t tmp = 0;
 	long r;
@@ -825,7 +829,11 @@ static int gfx_v8_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 	if (r)
 		goto err2;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait_timeout(f, false, timeout);
+#else
 	r = fence_wait_timeout(f, false, timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r == 0) {
 		DRM_ERROR("amdgpu: IB test timed out.\n");
 		r = -ETIMEDOUT;
@@ -845,7 +853,11 @@ static int gfx_v8_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 	}
 err2:
 	amdgpu_ib_free(adev, &ib, NULL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 err1:
 	amdgpu_gfx_scratch_free(adev, scratch);
 	return r;
@@ -1565,7 +1577,11 @@ static int gfx_v8_0_do_edc_gpr_workarounds(struct amdgpu_device *adev)
 {
 	struct amdgpu_ring *ring = &adev->gfx.compute_ring[0];
 	struct amdgpu_ib ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int r, i;
 	u32 tmp;
 	unsigned total_size, vgpr_offset, sgpr_offset;
@@ -1698,7 +1714,11 @@ static int gfx_v8_0_do_edc_gpr_workarounds(struct amdgpu_device *adev)
 	}
 
 	/* wait for the GPU to finish processing the IB */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait(f, false);
+#else
 	r = fence_wait(f, false);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r) {
 		DRM_ERROR("amdgpu: fence wait failed (%d).\n", r);
 		goto fail;
@@ -1719,7 +1739,11 @@ static int gfx_v8_0_do_edc_gpr_workarounds(struct amdgpu_device *adev)
 
 fail:
 	amdgpu_ib_free(adev, &ib, NULL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	return r;
 }
diff --git a/amd/amdgpu/sdma_v2_4.c b/amd/amdgpu/sdma_v2_4.c
index a26222d..8f8ceff 100644
--- a/amd/amdgpu/sdma_v2_4.c
+++ b/amd/amdgpu/sdma_v2_4.c
@@ -672,7 +672,11 @@ static int sdma_v2_4_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned index;
 	u32 tmp = 0;
 	u64 gpu_addr;
@@ -709,7 +713,11 @@ static int sdma_v2_4_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 	if (r)
 		goto err1;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait_timeout(f, false, timeout);
+#else
 	r = fence_wait_timeout(f, false, timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r == 0) {
 		DRM_ERROR("amdgpu: IB test timed out\n");
 		r = -ETIMEDOUT;
@@ -729,7 +737,11 @@ static int sdma_v2_4_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 
 err1:
 	amdgpu_ib_free(adev, &ib, NULL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 err0:
 	amdgpu_wb_free(adev, index);
 	return r;
diff --git a/amd/amdgpu/sdma_v3_0.c b/amd/amdgpu/sdma_v3_0.c
index 28db380..aa0d18a 100644
--- a/amd/amdgpu/sdma_v3_0.c
+++ b/amd/amdgpu/sdma_v3_0.c
@@ -875,7 +875,11 @@ static int sdma_v3_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned index;
 	u32 tmp = 0;
 	u64 gpu_addr;
@@ -912,7 +916,11 @@ static int sdma_v3_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 	if (r)
 		goto err1;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait_timeout(f, false, timeout);
+#else
 	r = fence_wait_timeout(f, false, timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r == 0) {
 		DRM_ERROR("amdgpu: IB test timed out\n");
 		r = -ETIMEDOUT;
@@ -931,7 +939,11 @@ static int sdma_v3_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 	}
 err1:
 	amdgpu_ib_free(adev, &ib, NULL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 err0:
 	amdgpu_wb_free(adev, index);
 	return r;
diff --git a/amd/amdgpu/si_dma.c b/amd/amdgpu/si_dma.c
index 14265c5..021fb71 100644
--- a/amd/amdgpu/si_dma.c
+++ b/amd/amdgpu/si_dma.c
@@ -274,7 +274,11 @@ static int si_dma_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 {
 	struct amdgpu_device *adev = ring->adev;
 	struct amdgpu_ib ib;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = NULL;
+#else
 	struct fence *f = NULL;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned index;
 	u32 tmp = 0;
 	u64 gpu_addr;
@@ -305,7 +309,11 @@ static int si_dma_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 	if (r)
 		goto err1;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	r = dma_fence_wait_timeout(f, false, timeout);
+#else
 	r = fence_wait_timeout(f, false, timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (r == 0) {
 		DRM_ERROR("amdgpu: IB test timed out\n");
 		r = -ETIMEDOUT;
@@ -325,7 +333,11 @@ static int si_dma_ring_test_ib(struct amdgpu_ring *ring, long timeout)
 
 err1:
 	amdgpu_ib_free(adev, &ib, NULL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 err0:
 	amdgpu_wb_free(adev, index);
 	return r;
diff --git a/amd/amdkcl/kcl_fence.c b/amd/amdkcl/kcl_fence.c
index cb97695..44fc448 100644
--- a/amd/amdkcl/kcl_fence.c
+++ b/amd/amdkcl/kcl_fence.c
@@ -3,7 +3,11 @@
 #include "kcl_common.h"
 
 #define CREATE_TRACE_POINTS
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+#include <trace/events/dma_fence.h>
+#else
 #include <trace/events/fence.h>
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 static atomic64_t fence_context_counter = ATOMIC64_INIT(0);
 u64 _kcl_fence_context_alloc(unsigned num)
@@ -13,9 +17,15 @@ u64 _kcl_fence_context_alloc(unsigned num)
 }
 EXPORT_SYMBOL(_kcl_fence_context_alloc);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+void
+_kcl_fence_init(struct dma_fence *fence, const struct dma_fence_ops *ops,
+	     spinlock_t *lock, u64 context, unsigned seqno)
+#else
 void
 _kcl_fence_init(struct fence *fence, const struct fence_ops *ops,
 	     spinlock_t *lock, u64 context, unsigned seqno)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	BUG_ON(!lock);
 	BUG_ON(!ops || !ops->wait || !ops->enable_signaling ||
@@ -29,18 +39,32 @@ _kcl_fence_init(struct fence *fence, const struct fence_ops *ops,
 	fence->seqno = seqno;
 	fence->flags = 0UL;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	trace_dma_fence_init(fence);
+#else
 	trace_fence_init(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 }
 EXPORT_SYMBOL(_kcl_fence_init);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static bool
+fence_test_signaled_any(struct dma_fence **fences, uint32_t count, uint32_t *idx)
+#else
 static bool
 fence_test_signaled_any(struct fence **fences, uint32_t count, uint32_t *idx)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	int i;
 
 	for (i = 0; i < count; ++i) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence = fences[i];
+		if (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags)) {
+#else
 		struct fence *fence = fences[i];
 		if (test_bit(FENCE_FLAG_SIGNALED_BIT, &fence->flags)) {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			if (idx)
 				*idx = i;
 			return true;
@@ -50,21 +74,36 @@ fence_test_signaled_any(struct fence **fences, uint32_t count, uint32_t *idx)
 }
 
 struct default_wait_cb {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence_cb base;
+#else
 	struct fence_cb base;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct task_struct *task;
 };
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void (*_kcl_fence_default_wait_cb)(struct dma_fence *fence, struct dma_fence_cb *cb);
+
+signed long
+kcl_fence_default_wait(struct dma_fence *fence, bool intr, signed long timeout)
+#else
 static void (*_kcl_fence_default_wait_cb)(struct fence *fence, struct fence_cb *cb);
 
 signed long
 kcl_fence_default_wait(struct fence *fence, bool intr, signed long timeout)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct default_wait_cb cb;
 	unsigned long flags;
 	signed long ret = timeout ? timeout : 1;
 	bool was_set;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags))
+#else
 	if (test_bit(FENCE_FLAG_SIGNALED_BIT, &fence->flags))
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		return ret;
 
 	spin_lock_irqsave(fence->lock, flags);
@@ -74,16 +113,30 @@ kcl_fence_default_wait(struct fence *fence, bool intr, signed long timeout)
 		goto out;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	was_set = test_and_set_bit(DMA_FENCE_FLAG_ENABLE_SIGNAL_BIT, &fence->flags);
+
+	if (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags))
+#else
 	was_set = test_and_set_bit(FENCE_FLAG_ENABLE_SIGNAL_BIT, &fence->flags);
 
 	if (test_bit(FENCE_FLAG_SIGNALED_BIT, &fence->flags))
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		goto out;
 
 	if (!was_set) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		trace_dma_fence_enable_signal(fence);
+#else
 		trace_fence_enable_signal(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		if (!fence->ops->enable_signaling(fence)) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_signal_locked(fence);
+#else
 			fence_signal_locked(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			goto out;
 		}
 	}
@@ -97,7 +150,11 @@ kcl_fence_default_wait(struct fence *fence, bool intr, signed long timeout)
 	cb.task = current;
 	list_add(&cb.base.node, &fence->cb_list);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	while (!test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags) && ret > 0) {
+#else
 	while (!test_bit(FENCE_FLAG_SIGNALED_BIT, &fence->flags) && ret > 0) {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (intr)
 			__set_current_state(TASK_INTERRUPTIBLE);
 		else
@@ -121,9 +178,15 @@ out:
 }
 EXPORT_SYMBOL(kcl_fence_default_wait);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+signed long
+_kcl_fence_wait_any_timeout(struct dma_fence **fences, uint32_t count,
+		       bool intr, signed long timeout, uint32_t *idx)
+#else
 signed long
 _kcl_fence_wait_any_timeout(struct fence **fences, uint32_t count,
 		       bool intr, signed long timeout, uint32_t *idx)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct default_wait_cb *cb;
 	signed long ret = timeout;
@@ -134,7 +197,11 @@ _kcl_fence_wait_any_timeout(struct fence **fences, uint32_t count,
 
 	if (timeout == 0) {
 		for (i = 0; i < count; ++i)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			if (dma_fence_is_signaled(fences[i])) {
+#else
 			if (fence_is_signaled(fences[i])) {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 				if (idx)
 					*idx = i;
 				return 1;
@@ -150,7 +217,11 @@ _kcl_fence_wait_any_timeout(struct fence **fences, uint32_t count,
 	}
 
 	for (i = 0; i < count; ++i) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence = fences[i];
+#else
 		struct fence *fence = fences[i];
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		if (fence->ops->wait != kcl_fence_default_wait) {
 			ret = -EINVAL;
@@ -158,8 +229,13 @@ _kcl_fence_wait_any_timeout(struct fence **fences, uint32_t count,
 		}
 
 		cb[i].task = current;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		if (dma_fence_add_callback(fence, &cb[i].base,
+				       _kcl_fence_default_wait_cb)) {
+#else
 		if (fence_add_callback(fence, &cb[i].base,
 				       _kcl_fence_default_wait_cb)) {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			/* This fence is already signaled */
 			if (idx)
 				*idx = i;
@@ -186,7 +262,11 @@ _kcl_fence_wait_any_timeout(struct fence **fences, uint32_t count,
 
 fence_rm_cb:
 	while (i-- > 0)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_remove_callback(fences[i], &cb[i].base);
+#else
 		fence_remove_callback(fences[i], &cb[i].base);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 err_free_cb:
 	kfree(cb);
@@ -195,17 +275,30 @@ err_free_cb:
 }
 EXPORT_SYMBOL(_kcl_fence_wait_any_timeout);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+signed long
+_kcl_fence_wait_timeout(struct dma_fence *fence, bool intr, signed long timeout)
+#else
 signed long
 _kcl_fence_wait_timeout(struct fence *fence, bool intr, signed long timeout)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	signed long ret;
 
 	if (WARN_ON(timeout < 0))
 		return -EINVAL;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	trace_dma_fence_wait_start(fence);
+#else
 	trace_fence_wait_start(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	ret = fence->ops->wait(fence, intr, timeout);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	trace_dma_fence_wait_end(fence);
+#else
 	trace_fence_wait_end(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return ret;
 }
 EXPORT_SYMBOL(_kcl_fence_wait_timeout);
diff --git a/amd/amdkcl/kcl_reservation.c b/amd/amdkcl/kcl_reservation.c
index dcc1329..6a6bb73 100644
--- a/amd/amdkcl/kcl_reservation.c
+++ b/amd/amdkcl/kcl_reservation.c
@@ -5,7 +5,11 @@ long _kcl_reservation_object_wait_timeout_rcu(struct reservation_object *obj,
 					 bool wait_all, bool intr,
 					 unsigned long timeout)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence;
+#else
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	unsigned seq, shared_count, i = 0;
 	long ret = timeout ? timeout : 1;
 
@@ -26,6 +30,20 @@ retry:
 			goto unlock_retry;
 
 		for (i = 0; i < shared_count; ++i) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			struct dma_fence *lfence = rcu_dereference(fobj->shared[i]);
+
+			if (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &lfence->flags))
+				continue;
+
+			if (!dma_fence_get_rcu(lfence))
+				goto unlock_retry;
+
+			if (dma_fence_is_signaled(lfence)) {
+				dma_fence_put(lfence);
+				continue;
+			}
+#else
 			struct fence *lfence = rcu_dereference(fobj->shared[i]);
 
 			if (test_bit(FENCE_FLAG_SIGNALED_BIT, &lfence->flags))
@@ -38,6 +56,7 @@ retry:
 				fence_put(lfence);
 				continue;
 			}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 			fence = lfence;
 			break;
@@ -45,11 +64,27 @@ retry:
 	}
 
 	if (!shared_count) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence_excl = rcu_dereference(obj->fence_excl);
+#else
 		struct fence *fence_excl = rcu_dereference(obj->fence_excl);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		if (read_seqcount_retry(&obj->seq, seq))
 			goto unlock_retry;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		if (fence_excl &&
+		    !test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence_excl->flags)) {
+			if (!dma_fence_get_rcu(fence_excl))
+				goto unlock_retry;
+
+			if (dma_fence_is_signaled(fence_excl))
+				dma_fence_put(fence_excl);
+			else
+				fence = fence_excl;
+		}
+#else
 		if (fence_excl &&
 		    !test_bit(FENCE_FLAG_SIGNALED_BIT, &fence_excl->flags)) {
 			if (!fence_get_rcu(fence_excl))
@@ -60,12 +95,17 @@ retry:
 			else
 				fence = fence_excl;
 		}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	}
 
 	rcu_read_unlock();
 	if (fence) {
 		ret = kcl_fence_wait_timeout(fence, intr, ret);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(fence);
+#else
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (ret > 0 && wait_all && (i + 1 < shared_count))
 			goto retry;
 	}
diff --git a/amd/scheduler/gpu_sched_trace.h b/amd/scheduler/gpu_sched_trace.h
index b961a1c..0ba630d 100644
--- a/amd/scheduler/gpu_sched_trace.h
+++ b/amd/scheduler/gpu_sched_trace.h
@@ -17,7 +17,11 @@ TRACE_EVENT(amd_sched_job,
 	    TP_STRUCT__entry(
 			     __field(struct amd_sched_entity *, entity)
 			     __field(struct amd_sched_job *, sched_job)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			     __field(struct dma_fence *, fence)
+#else
 			     __field(struct fence *, fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			     __field(const char *, name)
 			     __field(u32, job_count)
 			     __field(int, hw_job_count)
@@ -42,7 +46,11 @@ TRACE_EVENT(amd_sched_process_job,
 	    TP_PROTO(struct amd_sched_fence *fence),
 	    TP_ARGS(fence),
 	    TP_STRUCT__entry(
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		    __field(struct dma_fence *, fence)
+#else
 		    __field(struct fence *, fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		    ),
 
 	    TP_fast_assign(
diff --git a/amd/scheduler/gpu_scheduler.c b/amd/scheduler/gpu_scheduler.c
index 3ff25af..5356e19 100644
--- a/amd/scheduler/gpu_scheduler.c
+++ b/amd/scheduler/gpu_scheduler.c
@@ -32,7 +32,11 @@
 
 static bool amd_sched_entity_is_ready(struct amd_sched_entity *entity);
 static void amd_sched_wakeup(struct amd_gpu_scheduler *sched);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amd_sched_process_job(struct dma_fence *f, struct dma_fence_cb *cb);
+#else
 static void amd_sched_process_job(struct fence *f, struct fence_cb *cb);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 /* Initialize a given run queue struct */
 static void amd_sched_rq_init(struct amd_sched_rq *rq)
@@ -218,32 +222,56 @@ void amd_sched_entity_fini(struct amd_gpu_scheduler *sched,
 	kfifo_free(&entity->job_queue);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amd_sched_entity_wakeup(struct dma_fence *f, struct dma_fence_cb *cb)
+#else
 static void amd_sched_entity_wakeup(struct fence *f, struct fence_cb *cb)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amd_sched_entity *entity =
 		container_of(cb, struct amd_sched_entity, cb);
 	entity->dependency = NULL;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	amd_sched_wakeup(entity->sched);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amd_sched_entity_clear_dep(struct dma_fence *f, struct dma_fence_cb *cb)
+#else
 static void amd_sched_entity_clear_dep(struct fence *f, struct fence_cb *cb)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amd_sched_entity *entity =
 		container_of(cb, struct amd_sched_entity, cb);
 	entity->dependency = NULL;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(f);
+#else
 	fence_put(f);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 }
 
 static bool amd_sched_entity_add_dependency_cb(struct amd_sched_entity *entity)
 {
 	struct amd_gpu_scheduler *sched = entity->sched;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence * fence = entity->dependency;
+#else
 	struct fence * fence = entity->dependency;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct amd_sched_fence *s_fence;
 
 	if (fence->context == entity->fence_context) {
 		/* We can ignore fences from ourself */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(entity->dependency);
+#else
 		fence_put(entity->dependency);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		return false;
 	}
 
@@ -254,8 +282,21 @@ static bool amd_sched_entity_add_dependency_cb(struct amd_sched_entity *entity)
 		 * Fence is from the same scheduler, only need to wait for
 		 * it to be scheduled
 		 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		fence = dma_fence_get(&s_fence->scheduled);
+		dma_fence_put(entity->dependency);
+
+		entity->dependency = fence;
+		if (!dma_fence_add_callback(fence, &entity->cb,
+					amd_sched_entity_clear_dep))
+			return true;
+
+		/* Ignore it when it is already scheduled */
+		dma_fence_put(fence);
+#else
 		fence = fence_get(&s_fence->scheduled);
 		fence_put(entity->dependency);
+
 		entity->dependency = fence;
 		if (!fence_add_callback(fence, &entity->cb,
 					amd_sched_entity_clear_dep))
@@ -263,14 +304,23 @@ static bool amd_sched_entity_add_dependency_cb(struct amd_sched_entity *entity)
 
 		/* Ignore it when it is already scheduled */
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		return false;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (!dma_fence_add_callback(entity->dependency, &entity->cb,
+				amd_sched_entity_wakeup))
+		return true;
+
+	dma_fence_put(entity->dependency);
+#else
 	if (!fence_add_callback(entity->dependency, &entity->cb,
 				amd_sched_entity_wakeup))
 		return true;
 
 	fence_put(entity->dependency);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	return false;
 }
 
@@ -351,7 +401,11 @@ static void amd_sched_job_finish(struct work_struct *work)
 	sched->ops->free_job(s_job);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amd_sched_job_finish_cb(struct dma_fence *f, struct dma_fence_cb *cb)
+#else
 static void amd_sched_job_finish_cb(struct fence *f, struct fence_cb *cb)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amd_sched_job *job = container_of(cb, struct amd_sched_job,
 						 finish_cb);
@@ -385,10 +439,17 @@ void amd_sched_hw_job_reset(struct amd_gpu_scheduler *sched)
 
 	spin_lock(&sched->job_list_lock);
 	list_for_each_entry_reverse(s_job, &sched->ring_mirror_list, node) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		if (dma_fence_remove_callback(s_job->s_fence->parent, &s_job->s_fence->cb)) {
+			dma_fence_put(s_job->s_fence->parent);
+			s_job->s_fence->parent = NULL;
+		}
+#else
 		if (fence_remove_callback(s_job->s_fence->parent, &s_job->s_fence->cb)) {
 			fence_put(s_job->s_fence->parent);
 			s_job->s_fence->parent = NULL;
 		}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	}
 	atomic_set(&sched->hw_rq_count, 0);
 	spin_unlock(&sched->job_list_lock);
@@ -407,21 +468,35 @@ void amd_sched_job_recovery(struct amd_gpu_scheduler *sched)
 
 	list_for_each_entry_safe(s_job, tmp, &sched->ring_mirror_list, node) {
 		struct amd_sched_fence *s_fence = s_job->s_fence;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence;
+#else
 		struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		spin_unlock(&sched->job_list_lock);
 		fence = sched->ops->run_job(s_job);
 		atomic_inc(&sched->hw_rq_count);
 		if (fence) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			s_fence->parent = dma_fence_get(fence);
+			r = dma_fence_add_callback(fence, &s_fence->cb,
+					       amd_sched_process_job);
+#else
 			s_fence->parent = fence_get(fence);
 			r = fence_add_callback(fence, &s_fence->cb,
 					       amd_sched_process_job);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			if (r == -ENOENT)
 				amd_sched_process_job(fence, &s_fence->cb);
 			else if (r)
 				DRM_ERROR("fence add callback failed (%d)\n",
 					  r);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_put(fence);
+#else
 			fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		} else {
 			DRM_ERROR("Failed to run job!\n");
 			amd_sched_process_job(NULL, &s_fence->cb);
@@ -443,8 +518,13 @@ void amd_sched_entity_push_job(struct amd_sched_job *sched_job)
 	struct amd_sched_entity *entity = sched_job->s_entity;
 
 	trace_amd_sched_job(sched_job);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_add_callback(&sched_job->s_fence->finished, &sched_job->finish_cb,
+			       amd_sched_job_finish_cb);
+#else
 	fence_add_callback(&sched_job->s_fence->finished, &sched_job->finish_cb,
 			   amd_sched_job_finish_cb);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	wait_event(entity->sched->job_scheduled,
 		   amd_sched_entity_in(sched_job));
 }
@@ -508,7 +588,11 @@ amd_sched_select_entity(struct amd_gpu_scheduler *sched)
 	return entity;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amd_sched_process_job(struct dma_fence *f, struct dma_fence_cb *cb)
+#else
 static void amd_sched_process_job(struct fence *f, struct fence_cb *cb)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amd_sched_fence *s_fence =
 		container_of(cb, struct amd_sched_fence, cb);
@@ -518,7 +602,11 @@ static void amd_sched_process_job(struct fence *f, struct fence_cb *cb)
 	amd_sched_fence_finished(s_fence);
 
 	trace_amd_sched_process_job(s_fence);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(&s_fence->finished);
+#else
 	fence_put(&s_fence->finished);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	wake_up_interruptible(&sched->wake_up_worker);
 }
 
@@ -544,7 +632,11 @@ static int amd_sched_main(void *param)
 		struct amd_sched_entity *entity = NULL;
 		struct amd_sched_fence *s_fence;
 		struct amd_sched_job *sched_job;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		struct dma_fence *fence;
+#else
 		struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		wait_event_interruptible(sched->wake_up_worker,
 					 (!amd_sched_blocked(sched) &&
@@ -566,15 +658,25 @@ static int amd_sched_main(void *param)
 		fence = sched->ops->run_job(sched_job);
 		amd_sched_fence_scheduled(s_fence);
 		if (fence) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			s_fence->parent = dma_fence_get(fence);
+			r = dma_fence_add_callback(fence, &s_fence->cb,
+						   amd_sched_process_job);
+#else
 			s_fence->parent = fence_get(fence);
 			r = fence_add_callback(fence, &s_fence->cb,
 					       amd_sched_process_job);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			if (r == -ENOENT)
 				amd_sched_process_job(fence, &s_fence->cb);
 			else if (r)
 				DRM_ERROR("fence add callback failed (%d)\n",
 					  r);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_put(fence);
+#else
 			fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		} else {
 			DRM_ERROR("Failed to run job!\n");
 			amd_sched_process_job(NULL, &s_fence->cb);
diff --git a/amd/scheduler/gpu_scheduler.h b/amd/scheduler/gpu_scheduler.h
index 338d840..a4f35cc 100644
--- a/amd/scheduler/gpu_scheduler.h
+++ b/amd/scheduler/gpu_scheduler.h
@@ -29,7 +29,12 @@
 #else
 #include <linux/kfifo.h>
 #endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+#include <linux/dma-fence.h>
+#else
 #include <linux/fence.h>
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 struct amd_gpu_scheduler;
 struct amd_sched_rq;
@@ -51,8 +56,13 @@ struct amd_sched_entity {
 	atomic_t			fence_seq;
 	uint64_t                        fence_context;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence		*dependency;
+	struct dma_fence_cb		cb;
+#else
 	struct fence			*dependency;
 	struct fence_cb			cb;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 };
 
 /**
@@ -67,10 +77,17 @@ struct amd_sched_rq {
 };
 
 struct amd_sched_fence {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence                scheduled;
+	struct dma_fence                finished;
+	struct dma_fence_cb             cb;
+	struct dma_fence                *parent;
+#else
 	struct fence                    scheduled;
 	struct fence                    finished;
 	struct fence_cb                 cb;
 	struct fence                    *parent;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct amd_gpu_scheduler	*sched;
 	spinlock_t			lock;
 	void                            *owner;
@@ -80,15 +97,25 @@ struct amd_sched_job {
 	struct amd_gpu_scheduler        *sched;
 	struct amd_sched_entity         *s_entity;
 	struct amd_sched_fence          *s_fence;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence_cb		finish_cb;
+#else
 	struct fence_cb			finish_cb;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct work_struct		finish_work;
 	struct list_head		node;
 	struct delayed_work		work_tdr;
 };
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+extern const struct dma_fence_ops amd_sched_fence_ops_scheduled;
+extern const struct dma_fence_ops amd_sched_fence_ops_finished;
+static inline struct amd_sched_fence *to_amd_sched_fence(struct dma_fence *f)
+#else
 extern const struct fence_ops amd_sched_fence_ops_scheduled;
 extern const struct fence_ops amd_sched_fence_ops_finished;
 static inline struct amd_sched_fence *to_amd_sched_fence(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	if (f->ops == &amd_sched_fence_ops_scheduled)
 		return container_of(f, struct amd_sched_fence, scheduled);
@@ -104,8 +131,13 @@ static inline struct amd_sched_fence *to_amd_sched_fence(struct fence *f)
  * these functions should be implemented in driver side
 */
 struct amd_sched_backend_ops {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *(*dependency)(struct amd_sched_job *sched_job);
+	struct dma_fence *(*run_job)(struct amd_sched_job *sched_job);
+#else
 	struct fence *(*dependency)(struct amd_sched_job *sched_job);
 	struct fence *(*run_job)(struct amd_sched_job *sched_job);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	void (*timedout_job)(struct amd_sched_job *sched_job);
 	void (*free_job)(struct amd_sched_job *sched_job);
 };
diff --git a/amd/scheduler/sched_fence.c b/amd/scheduler/sched_fence.c
index a9991a7..3958b52 100644
--- a/amd/scheduler/sched_fence.c
+++ b/amd/scheduler/sched_fence.c
@@ -71,36 +71,66 @@ struct amd_sched_fence *amd_sched_fence_create(struct amd_sched_entity *entity,
 
 void amd_sched_fence_scheduled(struct amd_sched_fence *fence)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	int ret = dma_fence_signal(&fence->scheduled);
+
+	if (!ret)
+		DMA_FENCE_TRACE(&fence->scheduled, "signaled from irq context\n");
+	else
+		DMA_FENCE_TRACE(&fence->scheduled, "was already signaled\n");
+#else
 	int ret = fence_signal(&fence->scheduled);
 
 	if (!ret)
 		FENCE_TRACE(&fence->scheduled, "signaled from irq context\n");
 	else
 		FENCE_TRACE(&fence->scheduled, "was already signaled\n");
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 }
 
 void amd_sched_fence_finished(struct amd_sched_fence *fence)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	int ret = dma_fence_signal(&fence->finished);
+
+	if (!ret)
+		DMA_FENCE_TRACE(&fence->finished, "signaled from irq context\n");
+	else
+		DMA_FENCE_TRACE(&fence->finished, "was already signaled\n");
+#else
 	int ret = fence_signal(&fence->finished);
 
 	if (!ret)
 		FENCE_TRACE(&fence->finished, "signaled from irq context\n");
 	else
 		FENCE_TRACE(&fence->finished, "was already signaled\n");
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static const char *amd_sched_fence_get_driver_name(struct dma_fence *fence)
+#else
 static const char *amd_sched_fence_get_driver_name(struct fence *fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	return "amd_sched";
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static const char *amd_sched_fence_get_timeline_name(struct dma_fence *f)
+#else
 static const char *amd_sched_fence_get_timeline_name(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amd_sched_fence *fence = to_amd_sched_fence(f);
 	return (const char *)fence->sched->name;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static bool amd_sched_fence_enable_signaling(struct dma_fence *f)
+#else
 static bool amd_sched_fence_enable_signaling(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	return true;
 }
@@ -114,10 +144,18 @@ static bool amd_sched_fence_enable_signaling(struct fence *f)
  */
 static void amd_sched_fence_free(struct rcu_head *rcu)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *f = container_of(rcu, struct dma_fence, rcu);
+#else
 	struct fence *f = container_of(rcu, struct fence, rcu);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	struct amd_sched_fence *fence = to_amd_sched_fence(f);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(fence->parent);
+#else
 	fence_put(fence->parent);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	kmem_cache_free(sched_fence_slab, fence);
 }
 
@@ -129,7 +167,11 @@ static void amd_sched_fence_free(struct rcu_head *rcu)
  * This function is called when the reference count becomes zero.
  * It just RCU schedules freeing up the fence.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amd_sched_fence_release_scheduled(struct dma_fence *f)
+#else
 static void amd_sched_fence_release_scheduled(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amd_sched_fence *fence = to_amd_sched_fence(f);
 
@@ -143,14 +185,26 @@ static void amd_sched_fence_release_scheduled(struct fence *f)
  *
  * Drop the extra reference from the scheduled fence to the base fence.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static void amd_sched_fence_release_finished(struct dma_fence *f)
+#else
 static void amd_sched_fence_release_finished(struct fence *f)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct amd_sched_fence *fence = to_amd_sched_fence(f);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(&fence->scheduled);
+#else
 	fence_put(&fence->scheduled);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+const struct dma_fence_ops amd_sched_fence_ops_scheduled = {
+#else
 const struct fence_ops amd_sched_fence_ops_scheduled = {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	.get_driver_name = amd_sched_fence_get_driver_name,
 	.get_timeline_name = amd_sched_fence_get_timeline_name,
 	.enable_signaling = amd_sched_fence_enable_signaling,
@@ -163,7 +217,11 @@ const struct fence_ops amd_sched_fence_ops_scheduled = {
 	.release = amd_sched_fence_release_scheduled,
 };
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+const struct dma_fence_ops amd_sched_fence_ops_finished = {
+#else
 const struct fence_ops amd_sched_fence_ops_finished = {
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	.get_driver_name = amd_sched_fence_get_driver_name,
 	.get_timeline_name = amd_sched_fence_get_timeline_name,
 	.enable_signaling = amd_sched_fence_enable_signaling,
diff --git a/include/drm/ttm/ttm_bo_api.h b/include/drm/ttm/ttm_bo_api.h
index 9eb940d..b100a21 100644
--- a/include/drm/ttm/ttm_bo_api.h
+++ b/include/drm/ttm/ttm_bo_api.h
@@ -209,7 +209,11 @@ struct ttm_buffer_object {
 	 * Members protected by a bo reservation.
 	 */
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *moving;
+#else
 	struct fence *moving;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	struct drm_vma_offset_node vma_node;
 
diff --git a/include/drm/ttm/ttm_bo_driver.h b/include/drm/ttm/ttm_bo_driver.h
index f44567c..11583d1 100644
--- a/include/drm/ttm/ttm_bo_driver.h
+++ b/include/drm/ttm/ttm_bo_driver.h
@@ -305,7 +305,11 @@ struct ttm_mem_type_manager {
 	/*
 	 * Protected by @move_lock.
 	 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *move;
+#else
 	struct fence *move;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 };
 
 /**
@@ -1032,7 +1036,12 @@ extern void ttm_bo_free_old_node(struct ttm_buffer_object *bo);
  */
 
 extern int ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,
-				     struct fence *fence, bool evict,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				     struct dma_fence *fence,
+#else
+				     struct fence *fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
+				     bool evict,
 				     struct ttm_mem_reg *new_mem);
 
 /**
@@ -1047,7 +1056,12 @@ extern int ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,
  * immediately or hang it on a temporary buffer object.
  */
 int ttm_bo_pipeline_move(struct ttm_buffer_object *bo,
-			 struct fence *fence, bool evict,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			 struct dma_fence *fence,
+#else
+			 struct fence *fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
+			 bool evict,
 			 struct ttm_mem_reg *new_mem);
 
 /**
diff --git a/include/drm/ttm/ttm_execbuf_util.h b/include/drm/ttm/ttm_execbuf_util.h
index b620c31..6838d8d 100644
--- a/include/drm/ttm/ttm_execbuf_util.h
+++ b/include/drm/ttm/ttm_execbuf_util.h
@@ -114,6 +114,10 @@ extern int ttm_eu_reserve_buffers(struct ww_acquire_ctx *ticket,
 
 extern void ttm_eu_fence_buffer_objects(struct ww_acquire_ctx *ticket,
 					struct list_head *list,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+					struct dma_fence *fence);
+#else
 					struct fence *fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 #endif
diff --git a/include/kcl/kcl_fence.h b/include/kcl/kcl_fence.h
index cf18702..af88273 100644
--- a/include/kcl/kcl_fence.h
+++ b/include/kcl/kcl_fence.h
@@ -2,12 +2,32 @@
 #define AMDKCL_FENCE_H
 
 #include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+#include <linux/dma-fence.h>
+#else
 #include <linux/fence.h>
+#endif
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+signed long
+kcl_fence_default_wait(struct dma_fence *fence, bool intr, signed long timeout);
+#else
 signed long
 kcl_fence_default_wait(struct fence *fence, bool intr, signed long timeout);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 #if defined(BUILD_AS_DKMS)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+extern signed long _kcl_fence_wait_any_timeout(struct dma_fence **fences,
+				   uint32_t count, bool intr,
+				   signed long timeout, uint32_t *idx);
+extern u64 _kcl_fence_context_alloc(unsigned num);
+extern void _kcl_fence_init(struct dma_fence *fence, const struct dma_fence_ops *ops,
+	     spinlock_t *lock, u64 context, unsigned seqno);
+extern signed long _kcl_fence_wait_timeout(struct dma_fence *fence, bool intr,
+				signed long timeout);
+#else
 extern signed long _kcl_fence_wait_any_timeout(struct fence **fences,
 				   uint32_t count, bool intr,
 				   signed long timeout, uint32_t *idx);
@@ -16,7 +36,8 @@ extern void _kcl_fence_init(struct fence *fence, const struct fence_ops *ops,
 	     spinlock_t *lock, u64 context, unsigned seqno);
 extern signed long _kcl_fence_wait_timeout(struct fence *fence, bool intr,
 				signed long timeout);
-#endif
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
+#endif /* BUILD_AS_DKMS */
 
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0) && !defined(OS_NAME_RHEL_7_3)
 static inline bool fence_is_later(struct fence *f1, struct fence *f2)
@@ -28,9 +49,15 @@ static inline bool fence_is_later(struct fence *f1, struct fence *f2)
 }
 #endif /* LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0) */
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static inline signed long kcl_fence_wait_any_timeout(struct dma_fence **fences,
+				   uint32_t count, bool intr,
+				   signed long timeout, uint32_t *idx)
+#else
 static inline signed long kcl_fence_wait_any_timeout(struct fence **fences,
 				   uint32_t count, bool intr,
 				   signed long timeout, uint32_t *idx)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 #if defined(BUILD_AS_DKMS)
 	return _kcl_fence_wait_any_timeout(fences, count, intr, timeout, idx);
@@ -48,8 +75,13 @@ static inline u64 kcl_fence_context_alloc(unsigned num)
 #endif
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static inline void kcl_fence_init(struct dma_fence *fence, const struct dma_fence_ops *ops,
+	     spinlock_t *lock, u64 context, unsigned seqno)
+#else
 static inline void kcl_fence_init(struct fence *fence, const struct fence_ops *ops,
 	     spinlock_t *lock, u64 context, unsigned seqno)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 #if defined(BUILD_AS_DKMS)
 	return _kcl_fence_init(fence, ops, lock, context, seqno);
@@ -58,8 +90,13 @@ static inline void kcl_fence_init(struct fence *fence, const struct fence_ops *o
 #endif
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+static inline signed long kcl_fence_wait_timeout(struct dma_fence *fences, bool intr,
+					signed long timeout)
+#else
 static inline signed long kcl_fence_wait_timeout(struct fence *fences, bool intr,
 					signed long timeout)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 #if defined(BUILD_AS_DKMS)
 	return _kcl_fence_wait_timeout(fences, intr, timeout);
diff --git a/include/kcl/kcl_fence_array.h b/include/kcl/kcl_fence_array.h
index bb4401e..bd8ea6e 100644
--- a/include/kcl/kcl_fence_array.h
+++ b/include/kcl/kcl_fence_array.h
@@ -22,6 +22,57 @@
 #ifndef __LINUX_FENCE_ARRAY_H
 #define __LINUX_FENCE_ARRAY_H
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+#include <linux/dma-fence.h>
+
+/**
+ * struct dma_fence_array_cb - callback helper for fence array
+ * @cb: fence callback structure for signaling
+ * @array: reference to the parent fence array object
+ */
+struct dma_fence_array_cb {
+	struct dma_fence_cb cb;
+	struct dma_fence_array *array;
+};
+
+/**
+ * struct dma_fence_array - fence to represent an array of fences
+ * @base: fence base class
+ * @lock: spinlock for fence handling
+ * @num_fences: number of fences in the array
+ * @num_pending: fences in the array still pending
+ * @fences: array of the fences
+ */
+struct dma_fence_array {
+	struct dma_fence base;
+
+	spinlock_t lock;
+	unsigned num_fences;
+	atomic_t num_pending;
+	struct dma_fence **fences;
+};
+
+extern const struct dma_fence_ops dma_fence_array_ops;
+
+/**
+ * to_fence_array - cast a fence to a fence_array
+ * @fence: fence to cast to a fence_array
+ *
+ * Returns NULL if the fence is not a fence_array,
+ * or the fence_array otherwise.
+ */
+static inline struct dma_fence_array *to_fence_array(struct dma_fence *fence)
+{
+	if (fence->ops != &dma_fence_array_ops)
+		return NULL;
+
+	return container_of(fence, struct dma_fence_array, base);
+}
+
+struct dma_fence_array *fence_array_create(int num_fences, struct dma_fence **fences,
+					   u64 context, unsigned seqno,
+					   bool signal_on_any);
+#else
 #include <linux/fence.h>
 
 /**
@@ -71,5 +122,6 @@ static inline struct fence_array *to_fence_array(struct fence *fence)
 struct fence_array *fence_array_create(int num_fences, struct fence **fences,
 				       u64 context, unsigned seqno,
 				       bool signal_on_any);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 #endif /* __LINUX_FENCE_ARRAY_H */
diff --git a/ttm/ttm_bo.c b/ttm/ttm_bo.c
index 93bad84..5bd2c52 100644
--- a/ttm/ttm_bo.c
+++ b/ttm/ttm_bo.c
@@ -150,7 +150,11 @@ static void ttm_bo_release_list(struct kref *list_kref)
 
 	ttm_tt_destroy(bo->ttm);
 	atomic_dec(&bo->glob->bo_count);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(bo->moving);
+#else
 	fence_put(bo->moving);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (bo->resv == &bo->ttm_resv)
 		reservation_object_fini(&bo->ttm_resv);
 	mutex_destroy(&bo->wu_mutex);
@@ -430,20 +434,32 @@ static void ttm_bo_cleanup_memtype_use(struct ttm_buffer_object *bo)
 static void ttm_bo_flush_all_fences(struct ttm_buffer_object *bo)
 {
 	struct reservation_object_list *fobj;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence;
+#else
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int i;
 
 	fobj = reservation_object_get_list(bo->resv);
 	fence = reservation_object_get_excl(bo->resv);
 	if (fence && !fence->ops->signaled)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_enable_sw_signaling(fence);
+#else
 		fence_enable_sw_signaling(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	for (i = 0; fobj && i < fobj->shared_count; ++i) {
 		fence = rcu_dereference_protected(fobj->shared[i],
 					reservation_object_held(bo->resv));
 
 		if (!fence->ops->signaled)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			dma_fence_enable_sw_signaling(fence);
+#else
 			fence_enable_sw_signaling(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	}
 }
 
@@ -796,11 +812,19 @@ static int ttm_bo_add_move_fence(struct ttm_buffer_object *bo,
 				 struct ttm_mem_type_manager *man,
 				 struct ttm_mem_reg *mem)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence;
+#else
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int ret;
 
 	spin_lock(&man->move_lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	fence = dma_fence_get(man->move);
+#else
 	fence = fence_get(man->move);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	spin_unlock(&man->move_lock);
 
 	if (fence) {
@@ -810,7 +834,11 @@ static int ttm_bo_add_move_fence(struct ttm_buffer_object *bo,
 		if (unlikely(ret))
 			return ret;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(bo->moving);
+#else
 		fence_put(bo->moving);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		bo->moving = fence;
 	}
 
@@ -1290,7 +1318,11 @@ static int ttm_bo_force_list_clean(struct ttm_bo_device *bdev,
 {
 	struct ttm_mem_type_manager *man = &bdev->man[mem_type];
 	struct ttm_bo_global *glob = bdev->glob;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	struct dma_fence *fence;
+#else
 	struct fence *fence;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	int ret;
 
 	/*
@@ -1313,12 +1345,21 @@ static int ttm_bo_force_list_clean(struct ttm_bo_device *bdev,
 	spin_unlock(&glob->lru_lock);
 
 	spin_lock(&man->move_lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	fence = dma_fence_get(man->move);
+#else
 	fence = fence_get(man->move);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	spin_unlock(&man->move_lock);
 
 	if (fence) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		ret = dma_fence_wait(fence, false);
+		dma_fence_put(fence);
+#else
 		ret = fence_wait(fence, false);
 		fence_put(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		if (ret) {
 			if (allow_errors) {
 				return ret;
@@ -1347,7 +1388,11 @@ int ttm_bo_clean_mm(struct ttm_bo_device *bdev, unsigned mem_type)
 		       mem_type);
 		return ret;
 	}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(man->move);
+#else
 	fence_put(man->move);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	man->use_type = false;
 	man->has_type = false;
diff --git a/ttm/ttm_bo_util.c b/ttm/ttm_bo_util.c
index d705166..c414f30 100644
--- a/ttm/ttm_bo_util.c
+++ b/ttm/ttm_bo_util.c
@@ -661,7 +661,11 @@ void ttm_bo_kunmap(struct ttm_bo_kmap_obj *map)
 EXPORT_SYMBOL(ttm_bo_kunmap);
 
 int ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			      struct dma_fence *fence,
+#else
 			      struct fence *fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 			      bool evict,
 			      struct ttm_mem_reg *new_mem)
 {
@@ -691,8 +695,13 @@ int ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,
 		 * operation has completed.
 		 */
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(bo->moving);
+		bo->moving = dma_fence_get(fence);
+#else
 		fence_put(bo->moving);
 		bo->moving = fence_get(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		ret = ttm_buffer_object_transfer(bo, &ghost_obj);
 		if (ret)
@@ -723,7 +732,12 @@ int ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,
 EXPORT_SYMBOL(ttm_bo_move_accel_cleanup);
 
 int ttm_bo_pipeline_move(struct ttm_buffer_object *bo,
-			 struct fence *fence, bool evict,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+			 struct dma_fence *fence,
+#else
+			 struct fence *fence,
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
+			 bool evict,
 			 struct ttm_mem_reg *new_mem)
 {
 	struct ttm_bo_device *bdev = bo->bdev;
@@ -747,8 +761,13 @@ int ttm_bo_pipeline_move(struct ttm_buffer_object *bo,
 		 * operation has completed.
 		 */
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(bo->moving);
+		bo->moving = dma_fence_get(fence);
+#else
 		fence_put(bo->moving);
 		bo->moving = fence_get(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 		ret = ttm_buffer_object_transfer(bo, &ghost_obj);
 		if (ret)
@@ -778,16 +797,28 @@ int ttm_bo_pipeline_move(struct ttm_buffer_object *bo,
 		 */
 
 		spin_lock(&from->move_lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		if (!from->move || dma_fence_is_later(fence, from->move)) {
+			dma_fence_put(from->move);
+			from->move = dma_fence_get(fence);
+		}
+#else
 		if (!from->move || fence_is_later(fence, from->move)) {
 			fence_put(from->move);
 			from->move = fence_get(fence);
 		}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		spin_unlock(&from->move_lock);
 
 		ttm_bo_free_old_node(bo);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		dma_fence_put(bo->moving);
+		bo->moving = dma_fence_get(fence);
+#else
 		fence_put(bo->moving);
 		bo->moving = fence_get(fence);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 
 	} else {
 		/**
diff --git a/ttm/ttm_bo_vm.c b/ttm/ttm_bo_vm.c
index 99a15ce..0cde834 100644
--- a/ttm/ttm_bo_vm.c
+++ b/ttm/ttm_bo_vm.c
@@ -57,7 +57,11 @@ static int ttm_bo_vm_fault_idle(struct ttm_buffer_object *bo,
 	/*
 	 * Quick non-stalling check for idle.
 	 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	if (dma_fence_is_signaled(bo->moving))
+#else
 	if (fence_is_signaled(bo->moving))
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		goto out_clear;
 
 	/*
@@ -72,14 +76,22 @@ static int ttm_bo_vm_fault_idle(struct ttm_buffer_object *bo,
 #endif
 
 		up_read(&vma->vm_mm->mmap_sem);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+		(void) dma_fence_wait(bo->moving, true);
+#else
 		(void) fence_wait(bo->moving, true);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 		goto out_unlock;
 	}
 
 	/*
 	 * Ordinary wait.
 	 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	ret = dma_fence_wait(bo->moving, true);
+#else
 	ret = fence_wait(bo->moving, true);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	if (unlikely(ret != 0)) {
 		ret = (ret != -ERESTARTSYS) ? VM_FAULT_SIGBUS :
 			VM_FAULT_NOPAGE;
@@ -87,7 +99,11 @@ static int ttm_bo_vm_fault_idle(struct ttm_buffer_object *bo,
 	}
 
 out_clear:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	dma_fence_put(bo->moving);
+#else
 	fence_put(bo->moving);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 	bo->moving = NULL;
 
 out_unlock:
diff --git a/ttm/ttm_execbuf_util.c b/ttm/ttm_execbuf_util.c
index a80717b..5b755a3 100644
--- a/ttm/ttm_execbuf_util.c
+++ b/ttm/ttm_execbuf_util.c
@@ -179,7 +179,12 @@ int ttm_eu_reserve_buffers(struct ww_acquire_ctx *ticket,
 EXPORT_SYMBOL(ttm_eu_reserve_buffers);
 
 void ttm_eu_fence_buffer_objects(struct ww_acquire_ctx *ticket,
-				 struct list_head *list, struct fence *fence)
+				 struct list_head *list,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+				 struct dma_fence *fence)
+#else
+				 struct fence *fence)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) */
 {
 	struct ttm_validate_buffer *entry;
 	struct ttm_buffer_object *bo;
-- 
2.7.3

